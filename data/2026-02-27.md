<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Dynamic Survey of Soft Set Theory and Its Extensions](https://arxiv.org/abs/2602.21268)
*Takaaki Fujita,Florentin Smarandache*

Main category: cs.AI

TL;DR: 本书提供了软集合理论及其各类扩展/变体的综合性综述。


<details>
  <summary>Details</summary>
Motivation: 软集合理论作为参数化决策建模的重要数学框架，能够以结构化的方式表示不确定性。过去几十年来，该理论发展迅速，产生了众多变体，并与拓扑学、拟阵理论等其他数学领域建立了广泛联系，因此有必要对这一领域进行全面梳理和总结。

Method: 本书采用综述方式，系统阐述：(1) 软集合理论的核心定义；(2) 主要扩展变体，包括超软集合、超超软集合、树状软集合、双极软集合和动态软集合等；(3) 具有代表性的构造方法；(4) 与拓扑学、拟阵理论等其他领域的关联；(5) 当前发展的关键方向。

Result: 产出了一份全面整合软集合领域知识的权威参考文献，为读者提供：完整的理论基础与核心定义；各类变体及其特征的系统梳理；具有代表性和启发性的构造实例；跨领域应用的理论框架；前沿研究方向的明确指引。

Conclusion: 本书作为一部重要的综述著作，有效整合了软集合理论的多元化发展成果，为从事参数化决策建模和不确定性表示的研究人员与从业者提供了宝贵的学术资源，既涵盖了理论根基，又展示了应用前景，充分反映了该领域的当前研究动态与发展趋势。

Abstract: Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.

</details>


### [2] [A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives](https://arxiv.org/abs/2602.21351)
*Dmitrii Pantiukhin,Ivan Kuznetsov,Boris Shapkin,Antonia Anna Jost,Thomas Jung,Nikolay Koldunov*

Main category: cs.AI

TL;DR: 本文提出了PANGAEA-GPT，一个用于自主数据发现和分析的分层多智能体框架，旨在解决地球科学数据复用率低的挑战。


<details>
  <summary>Details</summary>
Motivation: 地球科学数据的快速增长带来了严重的可扩展性挑战；虽然PANGAEA等数据仓储承载大量数据集，但引用指标表明大量数据未被充分利用，限制了数据的复用性。

Method: 采用PANGAEA-GPT分层多智能体框架，实现了集中式Supervisor-Worker拓扑结构，具有严格的数据类型感知路由、沙盒化确定性代码执行以及通过执行反馈实现自我纠错等特性，使智能体能够诊断和解决运行时错误。

Result: 通过物理海洋学和生态学领域的用例场景，验证了系统能够在最少人工干预下执行复杂的多步骤工作流程，展示了查询和分析异构仓储数据的能力。

Conclusion: 该框架提供了通过协调智能体工作流程查询和分析异构仓储数据的方法论，为提高地球科学数据的可发现性和可复用性提供了有效解决方案。

Abstract: The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.

</details>


### [3] [Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information](https://arxiv.org/abs/2602.21496)
*Umid Suleymanov,Zaur Rajabov,Emil Mirzazada,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: 本文提出了SemSIEdit，一个推理时的隐私保护框架，通过代理编辑器迭代重写敏感语义信息，能在减少34.6%泄露的同时仅损失9.8%的效用，并发现了模型规模与安全性相关的差异化行为和推理悖论。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）面临语义敏感信息泄露的新威胁，包括推断敏感身份属性、生成有害内容或产生错误信息。现有的结构化个人身份信息防御技术已成熟，但LLMs在不破坏效用的前提下自我调节复杂的、上下文相关的敏感信息泄露能力仍是未解决的开放科学问题。

Method: 提出SemSIEdit推理时框架，其中包含一个代理"编辑器"，它迭代地批评并重写敏感文本片段，以保持叙事流畅性而非简单拒绝回答。该框架通过分析隐私-效用权衡来评估效果。

Result: 研究发现隐私-效用帕累托前沿：代理重写在所有三个语义敏感信息类别中将泄露减少34.6%，而边际效用损失仅为9.8%。同时发现了规模依赖的安全分歧：大型推理模型（如GPT-5）通过建设性扩展实现安全，而容量受限模型则采用破坏性截断。还识别了推理悖论：推理时的推理虽增加了基线风险（使模型能做出更深的敏感推断），但也赋予防御能力执行安全重写。

Conclusion: SemSIEdit框架证明了LLMs在推理时进行有效的敏感信息重写而不显著损害效用的可行性，揭示了模型规模、推理能力与安全性之间的复杂关系，为解决LLMs的语义敏感信息泄露问题提供了新的研究方向和实践路径。

Abstract: While defenses for structured PII are mature, Large Language Models (LLMs) pose a new threat: Semantic Sensitive Information (SemSI), where models infer sensitive identity attributes, generate reputation-harmful content, or hallucinate potentially wrong information. The capacity of LLMs to self-regulate these complex, context-dependent sensitive information leaks without destroying utility remains an open scientific question. To address this, we introduce SemSIEdit, an inference-time framework where an agentic "Editor" iteratively critiques and rewrites sensitive spans to preserve narrative flow rather than simply refusing to answer. Our analysis reveals a Privacy-Utility Pareto Frontier, where this agentic rewriting reduces leakage by 34.6% across all three SemSI categories while incurring a marginal utility loss of 9.8%. We also uncover a Scale-Dependent Safety Divergence: large reasoning models (e.g., GPT-5) achieve safety through constructive expansion (adding nuance), whereas capacity-constrained models revert to destructive truncation (deleting text). Finally, we identify a Reasoning Paradox: while inference-time reasoning increases baseline risk by enabling the model to make deeper sensitive inferences, it simultaneously empowers the defense to execute safe rewrites.

</details>


### [4] [Power and Limitations of Aggregation in Compound AI Systems](https://arxiv.org/abs/2602.21556)
*Nivasini Ananthakrishnan,Meena Jagadeesan*

Main category: cs.AI

TL;DR: 本文在委托-代理框架下理论分析了聚合操作在复合AI系统中的作用，揭示了聚合通过可行域扩展、支持域扩展和约束集收缩这三种机制扩展系统设计者可引出输出集合的条件。


<details>
  <summary>Details</summary>
Motivation: 在设计复合AI系统时，经常查询多个相同模型副本并聚合响应。由于模型同质性，需要研究聚合是否真能比单模型访问更大范围的输出集合，即聚合能否突破模型能力和提示工程的局限。

Method: 采用风格化的委托-代理框架进行理论分析，将系统设计者通过奖励函数部分引导代理输出的能力纳入模型，同时考虑提示工程和模型能力的限制。

Result: 识别出三种自然机制——可行域扩展、支持域扩展和约束集收缩——通过这些机制聚合可扩展可引出输出集。证明任何扩大可引出性的聚合必须实现这些机制之一，且强化版本提供了刻画可引出性扩展的充要条件。并提供了玩具参考生成任务中LLM的实证说明。

Conclusion: 本研究为理解复合AI系统能够在何种情况下克服模型能力不足和提示工程限制提供了理论基础，向系统性地刻画复合AI系统的能力界限迈出了重要一步。

Abstract: When designing compound AI systems, a common approach is to query multiple copies of the same model and aggregate the responses to produce a synthesized output. Given the homogeneity of these models, this raises the question of whether aggregation unlocks access to a greater set of outputs than querying a single model. In this work, we investigate the power and limitations of aggregation within a stylized principal-agent framework. This framework models how the system designer can partially steer each agent's output through its reward function specification, but still faces limitations due to prompt engineering ability and model capabilities. Our analysis uncovers three natural mechanisms -- feasibility expansion, support expansion, and binding set contraction -- through which aggregation expands the set of outputs that are elicitable by the system designer. We prove that any aggregation operation must implement one of these mechanisms in order to be elicitability-expanding, and that strengthened versions of these mechanisms provide necessary and sufficient conditions that fully characterize elicitability-expansion. Finally, we provide an empirical illustration of our findings for LLMs deployed in a toy reference-generation task. Altogether, our results take a step towards characterizing when compound AI systems can overcome limitations in model capabilities and in prompt engineering.

</details>


### [5] [The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems](https://arxiv.org/abs/2602.21745)
*Hyo Jin Kim*

Main category: cs.AI

TL;DR: ASIR勇气模型将真相披露形式化为相-动态框架中的状态转换，统一描述了人类压力下的沉默和AI约束下的输出扭曲，通过数学不等式定义了从抑制到表达的转换条件。


<details>
  <summary>Details</summary>
Motivation: 将真相披露从传统的人格特质问题重新解释为动态状态转换过程，旨在为人类勇气行为和AI对齐问题提供统一的理论框架，理解不同系统中真相披露的动力学机制。

Method: 构建相-动态框架，定义从抑制态(S0)到表达态(S1)的状态转换条件：λ(1+γ)+ψ > θ+φ，包含基线开放性、关系放大、内部压力和转换成本等参数；将该架构扩展至AI系统，引入反馈机制建模路径依赖和分歧效应，将系统行为解释为约束相空间内相互作用力的几何后果。

Result: 提供了统一结构解释人类压力沉默和AI偏好驱动扭曲的形式化理论，通过反馈扩展揭示了系统参数的递归再标定如何产生路径依赖，揭示了勇气和对齐在动力学结构中的本质联系。

Conclusion: ASIR勇气模型通过将真相披露重新构架为相空间中相互作用力的动力学过程，而非意图或人格特质，为理解风险环境下的真相披露提供了跨人类与AI系统的统一正式视角，为alignment研究提供了新的理论工具。

Abstract: We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative forces exceed inhibitory thresholds, expressed by the inequality lambda(1+gamma)+psi > theta+phi, where the terms represent baseline openness, relational amplification, accumulated internal pressure, and transition costs.
  Although initially formulated for human truth-telling under asymmetric stakes, the same phase-dynamic architecture extends to AI systems operating under policy constraints and alignment filters. In this context, suppression corresponds to constrained output states, while structural pressure arises from competing objectives, contextual tension, and recursive interaction dynamics. The framework therefore provides a unified structural account of both human silence under pressure and AI preference-driven distortion.
  A feedback extension models how transition outcomes recursively recalibrate system parameters, generating path dependence and divergence effects across repeated interactions. Rather than attributing intention to AI systems, the model interprets shifts in apparent truthfulness as geometric consequences of interacting forces within constrained phase space. By reframing courage and alignment within a shared dynamical structure, the ASIR Courage Model offers a formal perspective on truth-disclosure under risk across both human and artificial systems.

</details>


### [6] [fEDM+: A Risk-Based Fuzzy Ethical Decision Making Framework with Principle-Level Explainability and Pluralistic Validation](https://arxiv.org/abs/2602.21746)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: 该论文对模糊伦理决策框架(fEDM)进行了扩展，提出了fEDM+，引入可解释性模块(ETM)和多元主义语义验证框架，增强了决策的可解释性和利益相关者感知的验证能力。


<details>
  <summary>Details</summary>
Motivation: 原fEDM框架虽然具有形式化合理性和决策一致性，但存在两个关键缺陷：决策缺乏原则性解释能力，以及在价值伦理多元主义条件下鲁棒性不足。

Method: 在两个主要方向扩展fEDM：(1)引入可解释性和可追溯性模块(ETM)，将每个伦理决策规则与道德原则显式关联，为每个推荐行动计算加权原则贡献；(2)用多元主义语义验证框架替代单一参照验证，根据不同利益相关者的原则优先级和风险容忍度评估决策。

Result: fEDM+框架在保留形式化可验证性的同时，实现了决策解释透明化、多元利益相关者感知的验证，从而提高决策的鲁棒性和情境敏感性。

Conclusion: fEDM+可以作为伦理敏感型AI系统的监督与治理层，为AI伦理决策提供既可验证又可解释的框架支持，能够更好适应多元价值取向下实际应用需求。

Abstract: In a previous work, we introduced the fuzzy Ethical Decision-Making framework (fEDM), a risk-based ethical reasoning architecture grounded in fuzzy logic. The original model combined a fuzzy Ethical Risk Assessment module (fERA) with ethical decision rules, enabled formal structural verification through Fuzzy Petri Nets (FPNs), and validated outputs against a single normative referent. Although this approach ensured formal soundness and decision consistency, it did not fully address two critical challenges: principled explainability of decisions and robustness under ethical pluralism. In this paper, we extend fEDM in two major directions. First, we introduce an Explainability and Traceability Module (ETM) that explicitly links each ethical decision rule to the underlying moral principles and computes a weighted principle-contribution profile for every recommended action. This enables transparent, auditable explanations that expose not only what decision was made but why, and on the basis of which principles. Second, we replace single-referent validation with a pluralistic semantic validation framework that evaluates decisions against multiple stakeholder referents, each encoding distinct principle priorities and risk tolerances. This shift allows principled disagreement to be formally represented rather than suppressed, thus increasing robustness and contextual sensitivity. The resulting extended fEDM, called fEDM+, preserves formal verifiability while achieving enhanced interpretability and stakeholder-aware validation, making it suitable as an oversight and governance layer for ethically sensitive AI systems.

</details>


### [7] [Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem](https://arxiv.org/abs/2602.21814)
*Heejin Jo*

Main category: cs.AI

TL;DR: 该研究发现STAR推理框架能大幅提升大语言模型在隐式物理约束推理任务上的表现，从0%准确率提升至85%，结合用户画像和RAG上下文最终达到100%准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在"洗车问题"这一需要隐式物理约束推理的热门基准测试中持续失败，需要探究何种提示架构能够支持正确的推理能力。

Method: 采用变量隔离研究设计（n=20/条件，共6个条件，120次试验），使用Claude 3.5 Sonnet模型（温度0.7，top_p=1.0），系统性地评估STAR（情境-任务-行动-结果）推理框架、用户画像上下文和RAG上下文对推理性能的影响。

Result: STAR推理框架单独使用将准确率从0%提升至85%（p=0.001，Fisher精确检验，优势比13.22）；添加用户画像上下文提供额外10%增益；RAG上下文贡献额外5%增益；全栈条件下达到100%准确率。

Conclusion: 结构化推理脚手架——特别是推理前的强制目标明确化——对于隐式约束推理任务比上下文注入技术更为重要。

Abstract: Large language models consistently fail the "car wash problem," a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.

</details>


### [8] [Distill and Align Decomposition for Enhanced Claim Verification](https://arxiv.org/abs/2602.21857)
*Jabez Magomere,Elena Kochkina,Samuel Mensah,Simerjot Kaur,Fernando Acero,Arturo Oncevay,Charese H. Smiley,Xiaomo Liu,Manuela Veloso*

Main category: cs.AI

TL;DR: 本文提出了一种基于Group Relative Policy Optimization (GRPO)的强化学习方法，通过联合优化分解质量和验证器对齐，显著提升了复杂声明验证的性能。


<details>
  <summary>Details</summary>
Motivation: 复杂声明验证需要将句子分解为可验证的子声明，但现有方法难以在分解质量和验证性能之间实现有效对齐，导致分解质量与最终的验证表现不匹配。

Method: 提出将结构化顺序推理、基于教师蒸馏示例的监督微调和多目标奖励机制相结合的GRPO强化学习框架。该奖励机制平衡了格式合规性、验证器对齐和分解质量三个目标，联合优化分解过程和下游验证任务。

Result: 在六个评估设置中，训练得到的8B参数分解器将下游验证性能提升至71.75% macro-F1，分别比基于提示词的方法高出1.99%和6.24%，比现有强化学习方法高出5.84%。人工评估验证了生成的子声明具有高质量。

Conclusion: 该框架使得较小的语言模型能够通过联合优化验证准确性和分解质量，在声明验证任务上达到最先进水平。

Abstract: Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.

</details>


### [9] [ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices](https://arxiv.org/abs/2602.21858)
*Dezhi Kong,Zhengzhao Feng,Qiliang Liang,Hao Wang,Haofei Sun,Changpeng Yang,Yang Li,Peng Zhou,Shuai Nie,Hongzhen Wang,Linfeng Zhou,Hao Jia,Jiaming Xu,Runyu Shi,Ying Huang*

Main category: cs.AI

TL;DR: 提出ProactiveMobile基准，用于评估移动智能体的主动智能能力，包含3660个实例、14个真实场景和63个API函数池。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在移动智能体领域主要局限于反应式范式（仅执行显式用户命令），主动智能（自主预测需求并发起行动）是下一个前沿，但发展受限于缺乏能够处理现实世界复杂性并实现客观可执行评估的基准。

Method: 引入ProactiveMobile综合基准：将主动任务形式化为从四个维度的设备上下文信号推断潜在用户意图，并从63个API的函数池生成可执行函数序列；包含14个场景的3660个实例，采用多答案注释应对现实世界复杂性；由30名专家团队进行最终审核，确保事实准确性、逻辑一致性和行动可行性。

Result: 微调后的Qwen2.5-VL-7B-Instruct达到19.15%成功率，优于o1（15.71%）和GPT-5（7.39%）。表明主动性是当前MLLMs普遍缺乏但可学习的关键能力。

Conclusion: 主动智能是当前MLLMs的关键缺失能力，但通过学习和基准评估可得到改善，ProactiveMobile基准对于推动主动智能评估和研究具有重要意义。

Abstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.

</details>


### [10] [2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support](https://arxiv.org/abs/2602.21889)
*Otto Nyberg,Fausto Carcassi,Giovanni Cinà*

Main category: cs.AI

TL;DR: 本文提出了2-Step Agent计算框架，用于研究AI辅助决策对人类决策和结果的影响，揭示了错位先验信念可能导致决策支持反而恶化结果的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管AI模型在越来越多领域支持人类决策，但我们缺乏对这些技术应用效果的深入理解。

Method: 引入2-Step Agent通用计算框架，运用贝叶斯因果推断方法模拟：（1）预测如何影响理性贝叶斯代理人的信念，（2）信念变化如何影响下游决策和结果。通过仿真实验验证框架的有效性。

Result: 仿真研究表明，哪怕只是单个错位的先验信念，就足以使AI决策支持的下游结果比完全不使用决策支持时更差。

Conclusion: 研究揭示了AI驱动决策支持系统存在的多个潜在陷阱，强调了彻底的模型文档记录和恰当的用户培训的必要性。

Abstract: Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how this change in beliefs affects the downstream decision and subsequent outcome. Using this framework, we show by simulations how a single misaligned prior belief can be sufficient for decision support to result in worse downstream outcomes compared to no decision support. Our results reveal several potential pitfalls of AI-driven decision support and highlight the need for thorough model documentation and proper user training.

</details>


### [11] [Semantic Partial Grounding via LLMs](https://arxiv.org/abs/2602.22067)
*Giuseppe Canonaco,Alberto Pozanco,Daniel Borrajo*

Main category: cs.AI

TL;DR: SPG-LLM通过使用大语言模型分析PDDL描述，在grounding之前识别并排除不相关的对象、动作和谓词，在多个困难grounding基准上实现了数量级的加速，同时保持或改善了计划质量。


<details>
  <summary>Details</summary>
Motivation: 传统规划中的grounding步骤随着任务规模增大，grounded actions和atoms呈指数级增长，导致计算瓶颈。现有的部分grounding方法主要依赖关系特征或学习嵌入，未能有效利用PDDL描述中的文本和结构信息。

Method: 提出SPG-LLM方法，利用大语言模型分析领域文件和问题文件，在grounding之前启发式地识别可能不相关的对象、动作和谓词，从而显著减少需要grounded的任务规模。

Result: 在七个难以ground的基准测试上，SPG-LLM实现了更快grounding（通常快几个数量级），并且在某些域中获得了相当或更好的计划成本。

Conclusion: 通过结合大语言模型的语义理解能力和grounding过程，可以有效解决经典规划中的计算瓶颈问题，为大规模规划任务提供了新的解决思路。

Abstract: Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.

</details>


### [12] [Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts](https://arxiv.org/abs/2602.22070)
*Jessica Y. Bo,Lillio Mok,Ashton Anderson*

Main category: cs.AI

TL;DR: 研究发现，大型语言模型在决策中表现出不一致的偏见：在信任度评价中更偏好人类专家，但在激励化押注任务中却过度选择算法，即使算法表现更差。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地应用于需要处理来自人类专家和算法代理等多源信息的决策任务，理解它们如何权衡不同来源的信息变得至关重要。特别是考虑到人类决策者中已知的算法厌恶现象，研究LLMs是否也存在类似偏见具有重要现实意义。

Method: 研究评估了8个不同的LLMs，采用行为经济学的实验范式，通过两种呈现方式进行：（1）陈述偏好：通过直接查询对人类专家或算法的信任程度；（2）显示偏好：通过提供两者性能的上下文示例，要求在两者之间进行激励化押注决策。

Result: 在不同评估任务中，LLMs表现出矛盾的行为：当被要求评价人类专家和算法的可信度时，LLMs给出更高的评价给人类专家，这与人类受访者的先前结果一致；然而，当展示人类专家和算法的绩效并要求进行激励化押注时，LLMs不成比例地选择算法，即使它表现明显更差。

Conclusion: LLMs对人类和算法存在不一致的偏见，这种偏见在它们被部署到高风险场景时需要慎重考虑。此外，研究揭示了LLMs对任务呈现格式的敏感性，这应该在AI安全的评估稳健性中得到严格审查。

Abstract: Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety.

</details>


### [13] [Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning](https://arxiv.org/abs/2602.22094)
*Nguyen Cong Nhat Le,John G. Rogers,Claire N. Bonial,Neil T. Dantam*

Main category: cs.AI

TL;DR: 本文提出了一种基于Petri网可达性松弛的规划方法，支持不变量合成、目标不可达检测和约束更新，在处理计划变更和识别不可行性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 计划往往因情况变化或理解变化而需要调整，有时甚至根本不存在可行计划。现有规划方法主要关注可行情况下的一次性规划，缺乏对领域更新或不可行性检测的支持。识别不可行性对于确定何时需要调整需求至关重要。

Method: 提出Petri网可达性松弛技术，实现鲁棒的不变量合成、高效的目标不可达性检测以及有用的不可行性解释。结合增量约束求解器，支持目标和约束的动态更新。

Result: 实验表明，与基线相比：生成了相当数量的不变量；检测到多达2倍的不可行性；在一次性规划方面表现具有竞争力；在测试领域中，顺序计划更新的性能优于基线方法。

Conclusion: 所提出的方法在不变量合成和不可达性检测方面表现优异，能够有效处理计划的动态更新需求，为智能规划系统提供了更强大的可行性验证和解释能力。

Abstract: Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations. We further leverage incremental constraint solvers to support goal and constraint updates. Empirically, compared to baselines, our system produces a comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in the tested domains.

</details>
