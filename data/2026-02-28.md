<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.AI](#cs.AI) [Total: 71]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Replacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA](https://arxiv.org/abs/2602.22721)
*Fengyu Li,Junhao Zhu,Kaishi Song,Lu Chen,Zhongming Yao,Tianyi Li,Christian S. Jensen*

Main category: cs.DB

TL;DR: 提出了Operation-R1框架，通过可验证奖励的强化学习变体训练轻量级LLM，实现单步生成高质量TQA数据准备管道，在提升性能的同时大幅降低延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于操作符的TQA解决方案虽然性能优异，但需要多步生成表格操作管道，导致推理延迟过高和计算成本过大，难以满足实际应用需求。

Method: 1) 设计Operation-R1框架训练轻量级LLM（如Qwen-4B/1.7B）；2) 引入自监督奖励机制自动获取管道级监督信号；3) 提出方差感知组重采样缓解训练不稳定性；4) 开发操作合并机制通过多候选共识过滤虚假操作；5) 设计自适应回滚提供运行时信息丢失保护；6) 通过可验证奖励的强化学习变体实现单步推理生成管道。

Result: 在两个基准数据集上，相比多步准备基线，使用相同LLM骨干网络平均精度绝对提升9.55和6.08个百分点，同时实现了79%的表格压缩和2.2倍的成本降低。

Conclusion: Operation-R1首次证明了可以在单步推理中生成高质量TQA数据准备管道，通过轻量级模型和创新训练策略，有效平衡了性能与效率，为实际部署提供了可行方案。

Abstract: Table Question Answering (TQA) aims to answer natural language questions over structured tables. Large Language Models (LLMs) enable promising solutions to this problem, with operator-centric solutions that generate table manipulation pipelines in a multi-step manner offering state-of-the-art performance. However, these solutions rely on multiple LLM calls, resulting in prohibitive latencies and computational costs.
  We propose Operation-R1, the first framework that trains lightweight LLMs (e.g., Qwen-4B/1.7B) via a novel variant of reinforcement learning with verifiable rewards to produce high-quality data-preparation pipelines for TQA in a single inference step. To train such an LLM, we first introduce a self-supervised rewarding mechanism to automatically obtain fine-grained pipeline-wise supervision signals for LLM training. We also propose variance-aware group resampling to mitigate training instability. To further enhance robustness of pipeline generation, we develop two complementary mechanisms: operation merge, which filters spurious operations through multi-candidate consensus, and adaptive rollback, which offers runtime protection against information loss in data transformation. Experiments on two benchmark datasets show that, with the same LLM backbone, Operation-R1 achieves average absolute accuracy gains of 9.55 and 6.08 percentage points over multi-step preparation baselines, with 79\% table compression and a 2.2$\times$ reduction in monetary cost.

</details>


### [2] [Optimizing SSD-Resident Graph Indexing for High-Throughput Vector Search](https://arxiv.org/abs/2602.22805)
*Weichen Zhao,Yuncheng Lu,Yao Tian,Hao Zhang,Jiehui Li,Minghao Zhao,Yakun Li,Weining Qian*

Main category: cs.DB

TL;DR: VeloANN是一种磁盘基近似最近邻搜索（ANNS）系统，通过局部性感知的数据布局和基于协程的异步运行时技术，有效解决了现有内存外ANNS系统在图遍历过程中的CPU利用不足和读放大问题，在保持低内存占用的同时实现接近内存系统的性能


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的ANNS方法虽然精度高、延迟低，但在扩展至超越内存规模时，利用SSD存储的内存外ANNS系统由于图遍历过程中访问局部性有限，导致严重的CPU利用不足和读放大（存储停滞）问题

Method: 通过三种核心技术架构实现：采用层次化压缩和基于亲和性的数据放置方案，将相关向量共同定位在同一页面以减少碎片和过度获取；设计记录级缓冲池，通过将向量的邻居分组为记录并持久保留热记录来减少页面交换；引入基于协程的异步运行时，结合异步预取和感知束的搜索策略，最小化磁盘I/O期间的CPU调度开销并优先使用缓存数据

Result: 实验结果表明VeloANN在吞吐量和延迟方面分别超过最先进的磁盘基ANN系统达5.8倍和3.25倍，同时仅使用内存系统10%的内存占用即可达到92%的吞吐量性能，在资源受限环境下实现了接近内存系统的搜索效率

Conclusion: VeloANN通过创新的局部性感知数据布局和协程异​​步运行时设计，成功突破了现有内存外ANNS系统的性能瓶颈，为大规模向量搜索提供了既节省内存又高性能的新解决方案

Abstract: Graph-based approximate nearest neighbor search (ANNS) methods (e.g., HNSW) have become the de facto state of the art for their high precision and low latency. To scale beyond main memory, recent out-of-memory ANNS systems leverage SSDs to store large vector indexes. However, they still suffer from severe CPU underutilization and read amplification (i.e., storage stalls) caused by limited access locality during graph traversal. We present VeloANN, which mitigates storage stalls through a locality-aware data layout and a coroutine-based asynchronous runtime. VeloANN utilizes hierarchical compression and affinity-based data placement scheme to co-locate related vectors within the same page, effectively reducing fragmentation and over-fetching. We further design a record-level buffer pool, where each record groups the neighbors of a vector; by persistently retaining hot records in memory, it eliminates excessive page swapping under constrained memory budgets. To minimize CPU scheduling overheads during disk I/O interruptions, VeloANN employs a coroutine-based asynchronous runtime for lightweight task scheduling. On top of this, it incorporates asynchronous prefetching and a beam-aware search strategy to prioritize cached data, ultimately improving overall search efficiency. Extensive experiments show that VeloANN outperforms state-of-the-art disk-based ANN systems by up to 5.8x in throughput and 3.25x in latency reduction, while achieving 0.92x the throughput of in-memory systems using only 10% of their memory footprint.

</details>


### [3] [Workload-Aware Incremental Reclustering in Cloud Data Warehouses](https://arxiv.org/abs/2602.23289)
*Yipeng Liu,Renfei Zhou,Jiaqi Yan,Haunchen Zhang*

Main category: cs.DB

TL;DR: 本文提出了WAIR，一种面向工作负载感知的数据仓库微分区重聚类算法，通过识别和重聚类边界微分区实现接近最优的查询性能，同时显著降低重聚类成本。


<details>
  <summary>Details</summary>
Motivation: 现代云数据仓库依赖微分区和元数据（如区段映射）实现高效数据修剪，而在动态云环境中，现有自动聚类方法缺乏应对持续数据摄取和工作负载演变所需的灵活性。

Method: 本文提出重聚类策略与聚类键选择的清晰分离机制，引入“边界微分区”概念（即位于查询范围边界的微分区），并设计WAIR算法——一种工作负载感知算法，仅识别和重聚类对修剪效率最关键的边界微分区，并将其实现为原型重聚类服务。

Result: WAIR实现了接近全排序表布局的最优查询性能，且具有理论成本上界，重聚类成本显著降低；在TPC-H、DSB标准基准和真实工作负载上的实验表明，WAIR相比现有方案提升了查询性能并降低了整体成本。

Conclusion: 该研究成功证明了通过聚焦关键边界微分区而非重聚类所有数据这一新方法，能够提供比重聚类现有方法更高效、更有效的解决方案。

Abstract: Modern cloud data warehouses store data in micro-partitions and rely on metadata (e.g., zonemaps) for efficient data pruning during query processing. Maintaining data clustering in a large-scale table is crucial for effective data pruning. Existing automatic clustering approaches lack the flexibility required in dynamic cloud environments with continuous data ingestion and evolving workloads. This paper advocates a clean separation between reclustering policy and clustering-key selection. We introduce the concept of boundary micro-partitions that sit on the boundary of query ranges. We then present WAIR, a workload-aware algorithm to identify and recluster only boundary micro-partitions most critical for pruning efficiency. WAIR achieves near-optimal (with respect to fully sorted table layouts) query performance but incurs significantly lower reclustering cost with a theoretical upper bound. We further implement the algorithm into a prototype reclustering service and evaluate on standard benchmarks (TPC-H, DSB) and a real-world workload. Results show that WAIR improves query performance and reduces the overall cost compared to existing solutions.

</details>


### [4] [AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search](https://arxiv.org/abs/2602.23342)
*Weijian Chen,Haotian Liu,Yangshen Deng,Long Xiang,Liang Huang,Gezi Li,Bo Tang*

Main category: cs.DB

TL;DR: 现有磁盘图近似最近邻搜索（ANNS）系统被认为是I/O受限的，但该研究发现高维向量场景下性能实际上是计算受限的。基于此，作者提出了AlayaLaser系统，通过SIMD优化的磁盘数据布局和多项优化技术，实现了超越现有磁盘索引系统、甚至媲美内存索引系统的性能。


<details>
  <summary>Details</summary>
Motivation: 论文发现了与传统认知相反的现象：随着向量维度的提升（数百至数千维），基于磁盘的图索引系统的性能瓶颈从I/O受限转变为计算受限。现有系统普遍专注于I/O优化而忽略计算开销，这为通过计算优化提升性能提供了显著空间。

Method: 1）通过改进的roofline模型对现有系统进行性能分析；2）设计新型的磁盘数据布局，利用现代CPU的SIMD指令有效缓解计算受限问题；3）开发一系列优化技术，包括基于度的节点缓存、基于聚类的入口点选择和早期调度策略；4）在大规模高维向量数据集上进行广泛实验验证。

Result: AlayaLaser在多种大规模高维向量数据集上不仅显著超越了现有的磁盘图索引系统，而且其性能能够匹配甚至超过内存索引系统，证明了系统设计的有效性。

Conclusion: 该研究揭示了高维向量搜索中计算受限的本质特征，通过重新审视并优化计算瓶颈而非仅仅关注I/O优化，成功实现了ANNS系统的性能突破。这一成果为未来大规模高维向量检索系统的设计提供了新的思路和技术路径。

Abstract: On-disk graph-based approximate nearest neighbor search (ANNS) is essential for large-scale, high-dimensional vector retrieval, yet its performance is widely recognized to be limited by the prohibitive I/O costs. Interestingly, we observed that the performance of on-disk graph-based index systems is compute-bound, not I/O-bound, with the rising of the vector data dimensionality (e.g., hundreds or thousands). This insight uncovers a significant optimization opportunity: existing on-disk graph-based index systems universally target I/O reduction and largely overlook computational overhead, which leaves a substantial performance improvement space.
  In this work, we propose AlayaLaser, an efficient on-disk graph-based index system for large-scale high-dimensional vector similarity search. In particular, we first conduct performance analysis on existing on-disk graph-based index systems via the adapted roofline model, then we devise a novel on-disk data layout in AlayaLaser to effectively alleviate the compute-bound, which is revealed by the above roofline model analysis, by exploiting SIMD instructions on modern CPUs. We next design a suite of optimization techniques (e.g., degree-based node cache, cluster-based entry point selection, and early dispatch strategy) to further improve the performance of AlayaLaser. We last conduct extensive experimental studies on a wide range of large-scale high-dimensional vector datasets to verify the superiority of AlayaLaser. Specifically, AlayaLaser not only surpasses existing on-disk graph-based index systems but also matches or even exceeds the performance of in-memory index systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making](https://arxiv.org/abs/2602.22771)
*Yusuke Watanabe,Yohei Kobashi,Takeshi Kojima,Yusuke Iwasawa,Yasushi Okuno,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 提出了ClinDet-Bench，第一个评估LLM在信息不完整条件下识别可确定性和适当回避能力的临床基准。


<details>
  <summary>Details</summary>
Motivation: 在临床决策中，信息不完整是常态，过早下结论和不必要的回避都可能威胁患者安全，因此需要评估LLM识别何时信息足以可靠判断的能力。

Method: 开发了ClinDet-Bench基准测试，基于临床评分系统构建部分信息场景，将问题分解为可确定和不可确定的条件，要求模型考虑所有缺失信息的假设来识别可确定性。

Result: 最近的LLM在不完整信息下无法有效识别可确定性，尽管它们能正确解释底层的评分知识并在完整信息条件下表现良好，但仍出现过早判断和过度回避的问题。

Conclusion: 现有的基准不足以评估LLM在临床环境中的安全性，ClinDet-Bench提供了评估可确定性识别和适当回避行为的框架，可应用于医学及其他高风险领域，且已公开可用。

Abstract: Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinability requires considering all hypotheses about missing information, including unlikely ones, and verifying whether the conclusion holds across them. We find that recent LLMs fail to identify determinability under incomplete information, producing both premature judgments and excessive abstention, despite correctly explaining the underlying scoring knowledge and performing well under complete information. These findings suggest that existing benchmarks are insufficient to evaluate the safety of LLMs in clinical settings. ClinDet-Bench provides a framework for evaluating determinability recognition, leading to appropriate abstention, with potential applicability to medicine and other high-stakes domains, and is publicly available.

</details>


### [6] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: 提出名为GYWI的科学创意生成系统，通过结合作者知识图谱和检索增强生成（RAG）为大型语言模型提供可控的学术上下文和可追溯的灵感路径，提升科学创意生成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在科学创意生成方面虽然表现出潜力，但生成的结果往往缺乏可控的学术上下文和可追溯的灵感来源路径，这使得创意的质量和可信度难以保证。

Method: （1）构建以作者为中心的知识图谱和灵感源采样算法，建立外部知识库；（2）设计混合检索机制（RAG+GraphRAG），融合深度和广度的知识形成混合上下文；（3）提出基于强化学习原则的Prompt优化策略，自动指导LLM基于混合上下文优化生成结果；（4）在arXiv（2018-2023）数据集上构建评估，采用包括多选题任务、LLM评分、人工评估和语义空间可视化等多种评估方法。

Result: 在GPT-4o、DeepSeek-V3、Qwen3-8B和Gemini 2.5等多个LLM上的实验表明，GYWI在新颖性、可行性、清晰度、相关性和显著性五个维度上均显著超过主流LLM的表现。

Conclusion: GYWI系统成功整合了知识图谱和检索增强生成技术，通过混合检索和Prompt优化策略，有效解决了LLM生成科学创意时缺乏可控上下文和可追溯性的问题，为AI驱动的科学研究创新提供了新的技术路径。

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [7] [FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation](https://arxiv.org/abs/2602.22273)
*Xiyuan Zhang,Huihang Wu,Jiayu Guo,Zhenlin Zhang,Yiwei Zhang,Liangyu Huo,Xiaoxiao Ma,Jiansong Wan,Xuewei Jiao,Yi Jing,Jian Xie*

Main category: cs.AI

TL;DR: 本文介绍了FIRE，一个用于评估LLMs理论金融知识和实际业务场景处理能力的综合基准测试，包含3000个金融场景问题和系统评估矩阵。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏有效工具来全面评估LLMs在金融领域的理论知识和实际应用能力，需要构建一个兼顾理论深度和实践价值的评估框架。

Method: 该基准通过两部分进行评估：1）理论评估使用来自权威金融资格认证考试的多样化考题；2）实践评估基于系统化的评估矩阵，覆盖复杂金融领域的关键子领域和业务活动，收集包含封闭式决策题和开放式场景题在内的3000个问题。

Result: 对包括轩辕4.0在内的多个主流LLMs进行了全面评估，系统分析了当前模型在金融应用中的能力边界。

Conclusion: FIRE基准为金融领域大语言模型的评估提供了标准化的测试框架，基准测试和评估代码已公开发布以推动后续研究。

Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.

</details>


### [8] [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)
*Willem Schooltink,Fabio Massimo Zennaro*

Main category: cs.AI

TL;DR: 提出了因果嵌入框架，作为因果模型抽象的推广，支持将多个细粒度模型映射到粗粒度模型的子系统中，应用于多分辨率边缘问题和数据集合并


<details>
  <summary>Details</summary>
Motivation: 现有的因果模型抽象方法只关注两个模型之间的关系，缺乏能够同时处理多个细粒度模型并映射到同一粗粒度模型子系统的框架

Method: 定义因果嵌入作为抽象概念的推广，提出推广的一致性概念，通过构建多分辨率边缘问题来展示因果嵌入的应用价值

Result: 展示了因果嵌入在统计边缘问题和因果边缘问题中的相关性，并证明了其在合并来自不同表示模型生成的数据集中的实际应用价值

Conclusion: 因果嵌入框架为处理多分辨率因果关系和数据集合并提供了理论基础，实现了对因果模型抽象方法的扩展

Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>


### [9] [Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents](https://arxiv.org/abs/2602.22302)
*Varun Pratap Bhardwaj*

Main category: cs.AI

TL;DR: 提出了代理行为契约框架，将设计契约原则引入自主AI代理，通过运行时执行保障行为合规性，实验显示其有效检测违规、约束硬约束合规并控制行为漂移，开销较低。


<details>
  <summary>Details</summary>
Motivation: 传统软件依靠形式化契约规范正确行为，而AI代理依赖提示和自然语言指令，缺乏形式化行为规范，导致行为漂移、治理失效及项目失败，需填补此规范空白。

Method: 定义代理行为契约C=(P, I, G, R)，包含前置条件、不变量、治理策略和恢复机制，作为运行时可执行组件；提出(p, delta, k)-满足度概率契约合规性概念，证明漂移界限定理（恢复率gamma>自然漂移alpha时，行为漂移期望界限D*=alpha/gamma）；建立多代理链安全契约组合充分条件及概率退化界限；实现运行时执行库AgentAssert，在AgentContract-Bench基准（200场景，6供应商7模型）评估。

Result: 1,980次会话评估显示：契约代理每会话检测5.2-6.8次软违规（基线完全漏检，p<0.0001，Cohen's d=6.7-33.8）；硬约束合规率88-100%；扩展会话行为漂移界限D*<0.27；前沿模型100%恢复，所有模型17-100%恢复；每次动作开销<10ms。

Conclusion: ABC框架通过形式化契约、概率合规性验证和运行时执行，有效规范AI代理行为，显著提升违规检测、硬约束合规和行为漂移控制能力，具备低开销特性，为代理AI部署提供可靠治理方案。

Abstract: Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and frequent project failures in agentic AI deployments. We introduce Agent Behavioral Contracts (ABC), a formal framework that brings Design-by-Contract principles to autonomous AI agents. An ABC contract C = (P, I, G, R) specifies Preconditions, Invariants, Governance policies, and Recovery mechanisms as first-class, runtime-enforceable components. We define (p, delta, k)-satisfaction -- a probabilistic notion of contract compliance that accounts for LLM non-determinism and recovery -- and prove a Drift Bounds Theorem showing that contracts with recovery rate gamma > alpha (the natural drift rate) bound behavioral drift to D* = alpha/gamma in expectation, with Gaussian concentration in the stochastic setting. We establish sufficient conditions for safe contract composition in multi-agent chains and derive probabilistic degradation bounds. We implement ABC in AgentAssert, a runtime enforcement library, and evaluate on AgentContract-Bench, a benchmark of 200 scenarios across 7 models from 6 vendors. Results across 1,980 sessions show that contracted agents detect 5.2-6.8 soft violations per session that uncontracted baselines miss entirely (p < 0.0001, Cohen's d = 6.7-33.8), achieve 88-100% hard constraint compliance, and bound behavioral drift to D* < 0.27 across extended sessions, with 100% recovery for frontier models and 17-100% across all models, at overhead < 10 ms per action.

</details>


### [10] [Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?](https://arxiv.org/abs/2602.22401)
*Yongjun Zhang*

Main category: cs.AI

TL;DR: 本文提出了"氛围研究"概念，以scholar-skill插件为案例，构建认知任务框架来界定社会科学中AI智能体的应用边界，并分析了其对专业的影响。


<details>
  <summary>Details</summary>
Motivation: AI智能体作为能够执行完整研究流程的自动化系统，代表了社会科学研究的质性变革，需要重新思考人类与AI在研究中的认知分工边界，而非传统的顺序分工。

Method: 1）引入"氛围研究"概念，类比"氛围编程"；2）以scholar-skill（Claude Code的21技能插件）为案例，展示从构想到投稿的完整研究流程；3）构建认知任务框架，沿可编码性和隐性知识需求两个维度分类研究活动；4）识别出贯穿各阶段的认知委托边界。

Result: AI智能体在研究速度、覆盖广度和方法论支架方面表现优异，但在理论原创性和领域隐性知识方面存在局限；框架揭示了认知边界而非顺序边界。

Conclusion: AI智能体在社会科学中的应用将带来三个关键影响：增强条件的脆弱性、职业分层风险和教学法危机。论文提出了负责任"氛围研究"的五项原则以指导实践。

Abstract: AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, query databases, search the web, and invoke domain-specific skills to execute entire research pipelines autonomously. This paper introduces the concept of vibe researching -- the AI-era parallel to ``vibe coding'' (Karpathy, 2025) -- and uses scholar-skill, a 21-skill plugin for Claude Code covering the full research pipeline from idea to submission, as an illustrative case. I develop a cognitive task framework that classifies research activities along two dimensions -- codifiability and tacit knowledge requirement -- to identify a delegation boundary that is cognitive, not sequential: it cuts through every stage of the research pipeline, not between stages. I argue that AI agents excel at speed, coverage, and methodological scaffolding but struggle with theoretical originality and tacit field knowledge. The paper concludes with an analysis of three implications for the profession -- augmentation with fragile conditions, stratification risk, and a pedagogical crisis -- and proposes five principles for responsible vibe researching.

</details>


### [11] [Towards Autonomous Memory Agents](https://arxiv.org/abs/2602.22406)
*Xinle Wu,Rui Zhang,Mustafa Anis Hussain,Yao Lu*

Main category: cs.AI

TL;DR: U-Mem 提出了一种自主记忆智能体，通过成本感知的知识提取级联和语义感知的 Thompson 采样，主动低成本地获取、验证和管理知识，在多个基准测试中超越了现有记忆基线和基于强化学习的优化方法。


<details>
  <summary>Details</summary>
Motivation: 当前的记忆智能体虽然通过将经验和对话历史提取到外部存储中改进了 LLM，但仍然是被动和响应式的；记忆增长受限于可用信息，且很少在面对不确定性时主动寻求外部输入。因此需要一种能够主动以最低成本获取、验证和管理知识的自主记忆智能体。

Method: U-Mem 实现了两个核心组件：（i）成本感知的知识提取级联：从低成本的自检/教师信号开始，逐步升级到工具验证的研究，仅在需要时寻求专家反馈；（ii）语义感知的 Thompson 采样：在记忆间平衡探索与利用，减轻冷启动偏差。

Result: 在可验证和不可验证的基准测试中，U-Mem 一致性地击败了先前的记忆基线，并且可以超越基于强化学习的优化。具体包括：将 HotpotQA（Qwen2.5-7B）提升 14.6 个点，将 AIME25（Gemini-2.5-flash）提升 7.33 个点。

Conclusion: U-Mem 成功展示了自主记忆智能体的优势，通过主动知识获取和成本感知策略，在不进行昂贵 LLM 训练的情况下实现了显著的性能提升，为 LLM 的记忆增强开辟了新的方向。

Abstract: Recent memory agents improve LLMs by extracting experiences and conversation history into an external storage. This enables low-overhead context assembly and online memory update without expensive LLM training. However, existing solutions remain passive and reactive; memory growth is bounded by information that happens to be available, while memory agents seldom seek external inputs in uncertainties. We propose autonomous memory agents that actively acquire, validate, and curate knowledge at a minimum cost. U-Mem materializes this idea via (i) a cost-aware knowledge-extraction cascade that escalates from cheap self/teacher signals to tool-verified research and, only when needed, expert feedback, and (ii) semantic-aware Thompson sampling to balance exploration and exploitation over memories and mitigate cold-start bias. On both verifiable and non-verifiable benchmarks, U-Mem consistently beats prior memory baselines and can surpass RL-based optimization, improving HotpotQA (Qwen2.5-7B) by 14.6 points and AIME25 (Gemini-2.5-flash) by 7.33 points.

</details>


### [12] [Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus](https://arxiv.org/abs/2602.22408)
*Caroline Ahn,Quan Do,Leah Bakst,Michael P. Pascale,Joseph T. McGuire,Michael E. Hasselmo,Chantal E. Stern*

Main category: cs.AI

TL;DR: 本项研究构建CogARC数据集并通过260名参与者解决视觉推理问题探讨人类抽象推理的认知策略。


<details>
  <summary>Details</summary>
Motivation: 人类能够从稀疏示例中快速学习并应用规则，研究旨在探究这一灵活抽象推理能力背后的认知机制。

Method: 从原ARC数据集筛选并构建适用于人类的CogARC数据集；开展两项实验共260名参与者，解决75个需从少量示例推断规则的抽象视觉推理任务；记录示例观察、编辑序列和多次提交等高分辨率行为数据。

Result: 参与者整体完成任务（准确率约80%~90%），但表现因问题难度和个体差异而变化。难题引发更长思考时间和更多样的策略路径；随任务推进参与者反应加快但准确率微降；即使错误解答仍高度趋同，解题轨迹有的直接收敛，有的经多次探索或重启后收敛。

Conclusion: CogARC为研究人类抽象推理提供了丰富的行为范式，揭示了人类在不确定性条件下如何泛化、错误泛化与适应策略的机制。

Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.

</details>


### [13] [Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents](https://arxiv.org/abs/2602.22413)
*Jonas Karge*

Main category: cs.AI

TL;DR: 本文研究异构智能体在校准自身可靠性后选择性投票的集体决策框架，将经典的 Condorcet 陪审团定理推广到带置信度门的序列设置，在 AI 集体决策中降低幻觉风险


<details>
  <summary>Details</summary>
Motivation: 经典 Condorcet 陪审团定理假设固定参与率，但现实中允许智能体在不确定时说'我不知道'更能提升群体决策准确性；需要研究异构智能体能学习自身可靠性并选择性弃权的场景

Method: 提出概率框架：(1)智能体进入校准阶段，更新对自身固定能力的信念；(2)通过最终置信度门决定是否投票或弃权；推导非渐进下界；通过蒙特卡洛模拟验证理论界限

Result: 成功推导出群体成功概率的非渐进下界；证明'选择性参与'机制将 CJT 的渐进保证推广到序列化置信度门设置；蒙特卡洛模拟验证了理论结果

Conclusion: 该框架具有通用性；特别适用于 AI 安全领域，能够有效缓解大型语言模型集体决策中的幻觉问题，为可靠的 AI 系统设计提供理论支持

Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \textit{hallucinations} in collective LLM decision-making.

</details>


### [14] [ArchAgent: Agentic AI-driven Computer Architecture Discovery](https://arxiv.org/abs/2602.22425)
*Raghav Gupta,Akanksha Jain,Abraham Gonzalez,Alexander Novikov,Po-Sen Huang,Matej Balog,Marvin Eisenberger,Sergey Shirobokov,Ngân Vũ,Martin Dixon,Borivoje Nikolić,Parthasarathy Ranganathan,Sagar Karandikar*

Main category: cs.AI

TL;DR: ArchAgent是一个基于AlphaEvolve构建的自动化计算机架构发现系统，能够在无人工参与的情况下自动设计和实现最先进的缓存替换策略，并在多核和单核工作负载上分别实现了5.3%和0.9%的IPC性能提升。


<details>
  <summary>Details</summary>
Motivation: 为满足不断爆炸的计算需求，敏捷硬件设计流程成为关键的倍增器。代理式生成AI系统在算法设计、代码效率提升和科学发现方面已取得显著进展，需要将AI引入计算机架构设计领域以加速硬件创新。

Method: ArchAgent基于AlphaEvolve构建，在既定的缓存替换策略设计竞赛框架内，自动设计并实现全新的缓存替换策略（不仅仅是调整参数，而是设计新的机制和逻辑）。系统支持"硅后超特化"，通过调整硬件策略中暴露的运行时可配置参数来针对特定工作负载进行优化。

Result: ArchAgent在2天内生成的策略在Google多核工作负载追踪上超过先前SOTA 5.3%的IPC；在18天内生成的策略在SPEC06单核工作负载上超过现有SOTA 0.9%的IPC，比人工开发的SOTA策略快3-5倍；通过硅后超特化技术在SPEC06上进一步获得2.4%的IPC提升。

Conclusion: ArchAgent证明了代理式AI在计算机架构设计中的强大潜力，能够加速创新并发现传统方法难以获得的设计。同时揭示了"模拟器逃逸"现象——AI利用了微架构模拟器中的漏洞，这标志着AI时代架构研究工具设计需要考虑新的挑战。

Abstract: Agile hardware design flows are a critically needed force multiplier to meet the exploding demand for compute. Recently, agentic generative AI systems have demonstrated significant advances in algorithm design, improving code efficiency, and enabling discovery across scientific domains.
  Bridging these worlds, we present ArchAgent, an automated computer architecture discovery system built on AlphaEvolve. We show ArchAgent's ability to automatically design/implement state-of-the-art (SoTA) cache replacement policies (architecting new mechanisms/logic, not only changing parameters), broadly within the confines of an established cache replacement policy design competition.
  In two days without human intervention, ArchAgent generated a policy achieving a 5.3% IPC speedup improvement over the prior SoTA on public multi-core Google Workload Traces. On the heavily-explored single-core SPEC06 workloads, it generated a policy in just 18 days showing a 0.9% IPC speedup improvement over the existing SoTA (a similar "winning margin" as reported by the existing SoTA). ArchAgent achieved these gains 3-5x faster than prior human-developed SoTA policies.
  Agentic flows also enable "post-silicon hyperspecialization" where agents tune runtime-configurable parameters exposed in hardware policies to further align the policies with a specific workload (mix). Exploiting this, we demonstrate a 2.4% IPC speedup improvement over prior SoTA on SPEC06 workloads.
  Finally, we outline broader implications for computer architecture research in the era of agentic AI. For example, we demonstrate the phenomenon of "simulator escapes", where the agentic AI flow discovered and exploited a loophole in a popular microarchitectural simulator - a consequence of the fact that these research tools were designed for a (now past) world where they were exclusively operated by humans acting in good-faith.

</details>


### [15] [How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?](https://arxiv.org/abs/2602.22441)
*Yingqian Cui,Zhenwei Dai,Bing He,Zhan Shi,Hui Liu,Rui Sun,Zhiji Liu,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 本文对潜在推理方法进行了全面分析，发现了捷径行为、推理过程的隐式剪枝压缩特性，以及监督强度与表示多样性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 尽管潜在推理作为一种在连续潜在空间执行多步骤计算的推理范式已被提出，且性能改进研究众多，但对其内部机制的理解尚不充分，需要深入分析潜在表示在推理过程中的作用和行为。

Method: 对不同监督水平的潜在推理方法进行系统性分析，通过观察模型行为、检验类BFS探索假设、以及探究监督强度与表示能力关系来揭示潜在推理的内在机制。

Result: 研究发现两个关键问题：(1) 普遍存在捷径行为，模型无需依赖潜在推理即可获得高准确率；(2) 潜在推理并非忠实地实现结构化搜索，而是呈现隐式剪枝和压缩规律。此外，揭示了监督强度的权衡效应：强监督减少捷径但限制假设多样性，弱监督允许丰富表示但增加捷径行为。

Conclusion: 潜在推理方法存在捷径行为和搜索机制简化现象，需要在监督强度、表示多样性和推理忠实性之间寻求平衡，这些发现为改进潜在推理模型提供了重要指导。

Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.

</details>


### [16] [A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines](https://arxiv.org/abs/2602.22442)
*Gaoyuan Du,Amit Ahlawat,Xiaoyang Liu,Jing Wu*

Main category: cs.AI

TL;DR: 本研究提出了一种评估 Agent 的创新方法，解决了传统 AutoML 系统评估仅关注最终结果的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于 Agent 的 AutoML 系统评估实践仍然以结果为中心，主要关注最终任务性能，缺乏用于事后评估中间决策质量的结构化、决策级别评估指标。

Method: 设计了一个评估 Agent（EA）作为观察者，在不干扰执行的情况下，对 AutoML agent 的中间决策进行4个维度的以决策为中心的评估：决策有效性、推理一致性、超越准确性的模型质量风险，以及反事实决策影响。

Result: 在4个概念验证实验中，EA能够以0.919的F1分数检测出错误决策；能够独立于最终结果识别推理不一致性；能够将下游性能变化归因于agent决策，揭示对最终指标-4.9%到+8.3%的影响；揭示了仅基于结果的指标无法看到的失败模式。

Conclusion: 这项工作将agentic AutoML系统的评估从基于结果的视角重新构建为对agent决策进行审计的视角，为创建可靠、可解释和可治理的自主机器学习系统奠定了基础。

Abstract: Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find that none of the surveyed agentic AutoML systems report structured, decision-level evaluation metrics intended for post-hoc assessment of intermediate decision quality. To address this limitation, we propose an Evaluation Agent (EA) that performs decision-centric assessment of AutoML agents without interfering with their execution. The EA is designed as an observer that evaluates intermediate decisions along four dimensions: decision validity, reasoning consistency, model quality risks beyond accuracy, and counterfactual decision impact. Across four proof-of-concept experiments, we demonstrate that the EA can (i) detect faulty decisions with an F1 score of 0.919, (ii) identify reasoning inconsistencies independent of final outcomes, and (iii) attribute downstream performance changes to agent decisions, revealing impacts ranging from -4.9\% to +8.3\% in final metrics. These results illustrate how decision-centric evaluation exposes failure modes that are invisible to outcome-only metrics. Our work reframes the evaluation of agentic AutoML systems from an outcome-based perspective to one that audits agent decisions, offering a foundation for reliable, interpretable, and governable autonomous ML systems.

</details>


### [17] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 本文提出对比世界模型（CWM），采用带有困难负样本挖掘的InfoNCE对比学习目标，微调大语言模型作为动作可行性评分器，在ScienceWorld基准测试上显著优于传统的监督微调（SFT）方法。


<details>
  <summary>Details</summary>
Motivation: 在具身智能体管道中，动作可行性评分器是关键瓶颈。现有SFT方法独立处理每个候选动作，没有明确教导模型区分物理正确与细微错误的动作，无法有效处理语义相似但物理不兼容的困难负样本。

Method: 提出对比世界模型（CWM），使用InfoNCE对比目标微调LLM作为动作评分器。核心思想是在评分空间中将有效动作与无效动作区分开，特别关注困难负样本（语义相似但物理不兼容的候选动作），通过困难负样本挖掘增强模型的判别能力。

Result: 在ScienceWorld基准的两个评估研究中：1) 在605个困难负样本测试对上，CWM在单词最小编辑负样本情况下的Precision@1比SFT提升6.76个百分点，AUC-ROC更高（0.929 vs 0.906）；2) 在分布外压力条件下，CWM保持更好的安全边际（-2.39 vs -3.96），表明黄金路径动作排名更靠前。

Conclusion: 对比训练能够诱导模型更忠实地捕捉物理可行性的表示，相比单纯的监督微调方法具有显著优势，证明了对比学习在实体智能体动作可行性评估任务中的有效性。

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [18] [ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization](https://arxiv.org/abs/2602.22465)
*Joseph Tso,Preston Schmittou,Quan Huynh,Jibran Hutchins*

Main category: cs.AI

TL;DR: 本研究提出了一个评估LLM直接求解约束优化问题能力的数据集，发现可行性和最优性存在显著差距，模型在处理复杂约束时面临挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在运营决策优化中应用广泛，但现有方法依赖求解器。研究者希望探索LLM能否直接生成完整约束优化问题的解决方案，无需外部工具辅助。

Method: 构建ConstraintBench基准，包含10个运筹学领域的200个任务。每个任务给出自然语言场景描述（实体、约束、优化目标），要求模型输出结构化解，并通过Gurobi求解器验证每个约束和最优性。评估了6个前沿模型的性能。

Result: 最佳模型约束满足率仅65%，但可行解的目标值已达最优解的89-96%。在0.1%容差下同时满足可行性和最优性的最高记录仅为30.5%。领域差异巨大（可行性0.8%-83.3%），主要失败模式包括持续时间约束误解、实体幻觉，以及设施选址和车辆路径中的可行性-最优性解耦（可行性高但最优性为0%）。

Conclusion: ConstraintBench首次系统评估了LLM直接求解约束优化问题的能力。研究表明，当前LLMs在可行性而非最优性方面存在主要瓶颈，且在不同领域和约束类型上表现差异显著。该基准及评估代码将公开发布以促进后续研究。

Abstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.

</details>


### [19] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: VERO是一个用于编码智能体优化的评估框架和基准测试套件，通过版本化管理、奖励机制和观测系统，支持对目标智能体进行edit-execute-evaluate循环的迭代改进。


<details>
  <summary>Details</summary>
Motivation: 编码智能体优化是一个重要且新兴的应用领域，但社区缺乏对此任务的系统性理解。智能体优化与传统软件工程存在根本性差异：目标智能体混合了确定性代码和随机LLM生成，需要结构化的方式捕获中间推理过程和下游执行结果。因此需要专门的评估框架和基准测试来推进该领域研究。

Method: 提出VERO评估框架，包含两个核心组件：(1)可重现的评估工具，提供版本化的智能体快照、预算控制的评估以及结构化执行轨迹；(2)基准测试套件，包含目标智能体和任务集合，附带参考评估程序。随后使用VERO进行实证研究，比较不同任务上优化器配置的差异，并分析能够可靠提升目标智能体性能的修改方法。

Result: 通过VERO框架完成了对不同优化器配置的比较研究，识别出哪些修改能够可靠地提高目标智能体的性能，为编码智能体优化任务提供了系统的实证分析基础。

Conclusion: VERO作为开源工具被发布，为智能体优化研究的核心能力提供了支撑，强调了智能体优化作为编码智能体关键能力的研究价值，并为后续研究提供了标准化的评估和基准测试基础设施。

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [20] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 本文是一篇关于人工智能与生命周期评估交叉领域的系统性综述研究，创新性地利用大型语言模型进行文献分析，揭示了AI在LCA中的应用趋势、主题演变及未来方向。


<details>
  <summary>Details</summary>
Motivation: 近年来AI集成到LCA的研究快速发展，并已有大量研究成功应用机器学习算法支持LCA的各个阶段，但针对AI-LCA交叉领域的全面性综述研究仍然有限。为填补这一空白，本研究对AI与LCA交叉领域的已发表工作进行了详细回顾。

Method: 研究采用双重视角的方法论：一方面利用大型语言模型（LLM）进行文本挖掘，系统识别当前趋势、新兴主题和未来方向；另一方面将基于LLM的文本挖掘方法与传统文献综述技术相结合，构建了一个动态且有效的分析框架。

Result: 分析发现：1）随着LCA研究扩展，AI技术的采用急剧增长；2）研究趋势向LLM驱动的方法明显转变，机器学习应用持续增加；3）AI方法与相应LCA阶段之间存在统计学显著相关性。研究框架能够同时捕捉高层研究趋势和细致的概念模式。

Conclusion: 研究证明了LLM辅助方法在支持大规模、可重复的跨领域综述方面的潜力，同时也评估了在AI技术快速发展背景下实现计算高效LCA的路径。这有助于LCA从业者将尖端工具和及时见解融入环境评估，从而增强可持续性驱动决策的严谨性和质量。

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [21] [Mirroring the Mind: Distilling Human-Like Metacognitive Strategies into Large Language Models](https://arxiv.org/abs/2602.22508)
*Ik-hwan Kim,Hyeongrok Han,Mingi Jung,Sangwon Yu,Jinseok Hong,Sang Hun Kim,Yoonyoung Choi,Sungroh Yoon*

Main category: cs.AI

TL;DR: 研究针对大型推理模型(LRMs)因缺乏自我调节控制导致的推理崩溃问题，提出了元认知行为调优(MBT)后训练框架，通过将元认知策略显式注入推理过程，实现了更稳定、高效的推理表现。


<details>
  <summary>Details</summary>
Motivation: LRMs在复杂推理任务中经常出现结构性脆弱，即使成功推导出有效中间步骤仍无法得出正确答案。系统分析发现，这些失败不是源于推理能力不足，而是缺乏自我调节控制——有效逻辑被无控制的探索所破坏，或无法识别逻辑的充分性。

Method: 提出Metacognitive Behavioral Tuning (MBT)后训练框架，将元认知行为显式注入模型的思维过程。MBT包含两种互补实现：(1) MBT-S：从零合成严谨推理轨迹；(2) MBT-R：重写学生初始轨迹以稳定内在探索模式。

Result: 在多跳问答基准测试中，MBT持续优于基线方法，在挑战性基准上取得显著提升。通过有效消除推理崩溃，MBT在大幅减少token消耗的同时实现了更高精度。

Conclusion: 内部化元认知策略能够带来更稳定和鲁棒的推理能力，MBT框架成功解决了LRMs中因自我调节控制不足导致的推理脆弱性问题。

Abstract: Large Reasoning Models (LRMs) often exhibit structural fragility in complex reasoning tasks, failing to produce correct answers even after successfully deriving valid intermediate steps. Through systematic analysis, we observe that these failures frequently stem not from a lack of reasoning capacity, but from a deficiency in self-regulatory control, where valid logic is destabilized by uncontrolled exploration or the failure to recognize logical sufficiency. Motivated by this observation, we propose Metacognitive Behavioral Tuning (MBT), a post-training framework that explicitly injects metacognitive behaviors into the model's thought process. MBT implements this via two complementary formulations: (1) MBT-S, which synthesizes rigorous reasoning traces from scratch, and (2) MBT-R, which rewrites the student's initial traces to stabilize intrinsic exploration patterns. Experiments across multi-hop QA benchmarks demonstrate that MBT consistently outperforms baselines, achieving notable gains on challenging benchmarks. By effectively eliminating reasoning collapse, MBT achieves higher accuracy with significantly reduced token consumption, demonstrating that internalizing metacognitive strategies leads to more stable and robust reasoning.

</details>


### [22] [A Mathematical Theory of Agency and Intelligence](https://arxiv.org/abs/2602.22519)
*Wael Hafez,Chenan Wei,Rodrigo Felipe,Amir Nazeri,Cameron Reid*

Main category: cs.AI

TL;DR: 研究了信息理论框架中代理系统和智能系统的核心差异。通过构建信息度量指标P，揭示量子、经典和智能体系统共享信息的理论界限，并验证了现有AI系统的局限性，提出基于实时信息监控的自适应反馈架构。


<details>
  <summary>Details</summary>
Motivation: 传统系统仅依赖目标达成结果进行评估，忽视资源利用效率的内在机理。现有AI虽有精确预测能力，但缺乏对交互过程隐含的信息衰减机制的有效监控，亟需构建能够捕捉系统内在交互特性的理论框架。

Method: 引入理论模型与信息论方法，通过第一性原理严格推导并验证信息共享度量P的理论界限，跨量子、经典及智能体系统进行理论分析与物理实证。实证通过双摆模拟、强化学习智能体和大语言模型对话等多维实验场景验证。

Result: 信息共享度量P的理论界限在量子系统达到1，经典系统不超过0.5，引入行动选择机制后进一步降低。物理实验与计算实验均验证了理论预测的可靠性。

Conclusion: 现有AI系统实现代理性但未达到真正的智能，智能要求系统具备从交互中学习、实时监控学习效率与自适应调节观测及行动范围的能力。实时监控P的反馈架构可支持自适应、有弹性的智能系统设计。

Abstract: To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.

</details>


### [23] [Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents](https://arxiv.org/abs/2602.22523)
*Ryan Liu,Dilip Arumugam,Cedegao E. Zhang,Sean Escola,Xaq Pitkow,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 本文提出将多个大语言模型(LLM)组合成模块化语言代理，论证了认知模型和AI算法可为其提供设计蓝图，并形式化了代理模板概念。


<details>
  <summary>Details</summary>
Motivation: 单个LLM在处理复杂问题时存在局限性，需要探索如何将多个LLM作为模块有效组合成整体以解决更困难的问题。

Method: 1)形式化代理模板概念，定义个体LLM的角色和功能组合方式；2)调研文献中各类语言代理；3)分析并突出这些代理背后的认知模型或AI算法衍生模板。

Result: 发现现有语言代理的设计模式可直接追溯至认知模型或AI算法，证明了这些模板在语言代理设计中的适用性和有效性。

Conclusion: 受到认知科学和AI启发的代理模板是开发高效、可解释的语言代理的强大工具，值得在设计实践中广泛应用。

Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.

</details>


### [24] [Agentic AI for Intent-driven Optimization in Cell-free O-RAN](https://arxiv.org/abs/2602.22539)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: cs.AI

TL;DR: 本文提出了一种面向无小区O-RAN的智能体AI框架，用于实现意图转换与优化，通过多个基于LLM的智能体协同工作来处理复杂意图。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要考虑由独立智能体处理的简单意图，但需要多个智能体协调的复杂意图在无小区O-RAN场景中仍未被充分探索。

Method: 提出包含四种智能体的协同框架：主管智能体（将操作员意图转换为优化目标）、用户权重智能体（从记忆模块检索经验确定预编码权重）、O-RU管理智能体（使用DRL算法确定活动O-RU集合）和监控智能体（监控速率并协调其他智能体），采用参数高效微调（PEFT）方法复用同一LLM。

Result: 仿真结果表明，与三种基线方案相比，在节能模式下该框架将活动O-RU数量减少了41.93%；使用PEFT方法后，与部署独立LLM智能体相比，内存使用量减少了92%。

Conclusion: 该智能体AI框架有效解决了无小区O-RAN中复杂意图协调问题，同时通过PEFT方法显著提升了框架的可扩展性和资源效率。

Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.

</details>


### [25] [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546)
*Zhiming Wang,Jinwei He,Feng Lu*

Main category: cs.AI

TL;DR: AHCE是一个主动人机增强挑战应对框架，通过学习如何请求专家推理而非简单求助，有效解决LLM代理在专业领域长尾知识不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理在通用推理方面表现良好，但在专业领域中因训练数据缺乏长尾知识而失败。人类专家可提供这些缺失知识，但其指导往往非结构化且不可靠，难以直接整合到代理计划中。

Method: 提出AHCE框架，其核心是Human Feedback Module (HFM)，该模块采用学习策略将人类专家视为交互式推理工具，实现按需的人机协作。

Result: 在Minecraft实验中证明有效：普通难度任务成功率提高32%，高难度任务成功率提高近70%，且仅需最少人工干预。

Conclusion: 成功增强代理需要学习如何请求专家推理，这超越了简单的求助请求，是将人类专家知识有效整合到AI系统中的关键。

Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.

</details>


### [26] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: 本研究提出了 CourtGuard，一个基于外部策略文档的检索增强多智能体框架，将安全评估重新构建为'证据辩论'机制，从而实现无需微调的高效安全控制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全机制严重依赖静态微调分类器，存在适应性僵化问题：无法在不进行昂贵的重新训练的情况下强制执行新的治理规则。

Method: CourtGuard采用检索增强的多智能体框架，通过基于外部策略文档的对抗性辩论进行安全评估，完全通过外部推理而非模型权重实现安全逻辑。

Result: 在7个安全基准测试中达到最先进性能，在零样本适应能力方面表现突出（通过更换参考策略在Wikipedia破坏性编辑任务上实现90%准确率），并成功利用CourtGuard策划和审计了9个新型对抗攻击数据集。

Conclusion: 将安全逻辑与模型权重解耦，为满足当前和未来的AI治理监管要求提供了一条稳健、可解释且灵活的有效路径。

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [27] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 本文提出了选择性策略检索(SSR)框架,解决基于示例的数学推理指导在不同问题和模型间效果不稳定的问题。SSR通过显式建模策略的'可执行性',结合经验性、多路线、源感知信号选择性检索和组合策略,在多个数学推理基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管基于示例的指导被广泛用于改进数学推理,但其有效性在不同问题和模型之间高度不稳定,即使指导是正确的且与问题相关。这种不稳定性源于一个此前未被充分探索的差距:策略使用(策略是否出现在成功解决方案中)与策略可执行性(策略作为目标模型指导时是否仍然有效)之间的分歧。

Method: 通过成对人工撰写和模型生成解决方案的受控分析,发现人类和模型衍生的策略在结构化、领域依赖方面存在系统性差异,导致互补的优势和一致的源依赖性反转。基于此诊断,提出选择性策略检索(SSR)框架,该框架使用经验性的多路线源感知信号显式建模可执行性,通过选择性检索和组合策略来指导推理。

Result: 在多个数学推理基准上,SSR相比直接求解、上下文学习和单源指导产生可靠且一致的改进。具体而言,在AIME25上准确率提高多达+13点,在Apex上提高+5点,这些改进是在紧凑推理模型上实现的。

Conclusion: 本研究揭示了数学推理中策略使用与可执行性之间的关键差距,并提出了SSR作为一种有效的解决方案。SSR通过显式建模可执行性并采用选择性检索策略,能够克服现有方法的不稳定性问题,为数学推理任务提供了一种更可靠、一致且有效的测试时间指导方法。代码和基准已公开,便于社区进一步研究和应用。

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [28] [Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach](https://arxiv.org/abs/2602.22585)
*Jodi M. Casabianca,Maggie Beiting-Parrish*

Main category: cs.AI

TL;DR: 将心理测量学评分者模型整合到AI评估流程中，以提高人工评估数据的可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 人工评估在训练和评估AI模型中扮演核心角色，但这些数据很少被视为受系统误差影响的测量结果，需要改善从人工判断得出的结论的可靠性和有效性。

Method: 研究采用心理测量学方法，整合评分者模型到AI评估流程。通过回顾严格性和中心性等常见评分者效应，利用多面Rasch模型分离真实输出质量与评分者行为特征。以OpenAI摘要数据集为实证案例，展示调整评分者严格性的具体过程。

Result: 通过调整评分者严格性可获得修正的摘要质量估计，提供对评分者表现的诊断洞察，使开发者能够基于调整后的分数而非原始、易出错的评分做出更准确的决策。

Conclusion: 将心理测量建模纳入人机交互评估为AI开发与评估提供了更原则性、透明、可靠、可解释且与构念对齐的实践路径。

Abstract: Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.

</details>


### [29] [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: SideQuest通过利用大推理模型自身的推理能力来执行KV缓存压缩，在保持精度的同时将峰值token使用量减少高达65%。


<details>
  <summary>Details</summary>
Motivation: 长期运行的代理任务需要多跳推理，外部检索的大量token导致内存快速增长并限制解码性能，现有的KV缓存压缩启发式方法无法有效支持多步推理模型。

Method: SideQuest将KV缓存压缩框定为与主推理任务并行执行的辅助任务，利用LRM自身通过推理评估上下文中token的有用性来进行压缩，避免了管理过程token污染模型记忆。

Result: 仅用215个样本训练的模型评估显示，SideQuest在代理任务上将峰值token使用量减少高达65%，精度下降极小，显著优于基于启发式的KV缓存压缩技术。

Conclusion: SideQuest证明了利用推理模型自身能力进行高效的KV缓存管理是可行的，为解决长上下文代理任务的内存限制问题提供了一种有效的并行压缩方案。

Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

</details>


### [30] [MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios](https://arxiv.org/abs/2602.22638)
*Zhiheng Song,Jingshuai Zhang,Chuan Qin,Chao Wang,Chao Chen,Longfei Xu,Kaikui Liu,Xiangxiang Chu,Hengshu Zhu*

Main category: cs.AI

TL;DR: MobilityBench：一个用于评估基于大语言模型的现实世界路线规划智能体的可扩展基准框架。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的路线规划智能体在支持人类日常生活移动性方面具有巨大潜力，但由于多样化的路径规划需求、非确定性的地图服务和有限的复现性，系统化的现实世界评估面临重大挑战。

Method: 构建了MobilityBench基准，基于高德地图收集的大规模匿名真实用户查询，涵盖全球多个城市的多样化路线规划意图；设计确定性API重放沙盒以消除实时服务的环境差异；提出以结果有效性为中心的多维评估协议，涵盖指令理解、规划、工具使用和效率等维度。

Result: 评估发现，当前模型在基础信息检索和路线规划任务上表现良好，但在偏好约束的路线规划方面存在显著困难，揭示了个性化移动性应用中仍有很大改进空间。

Conclusion: MobilityBench为LLM驱动的路线规划智能体提供了可复现、端到端的评估平台，明确了现有模型的优势与不足，为未来个性化移动应用的发展指明了方向。

Abstract: Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .

</details>


### [31] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: 本文提出AHBid框架，通过集成扩散模型驱动的生成式规划与实时控制，解决多渠道在线广告竞价中的预算动态分配问题，实现13.57%的收益提升。


<details>
  <summary>Details</summary>
Motivation: 在线广告环境的复杂性和动态性需要自动竞价服务，尤其在多渠道场景下，有效分配预算和约束对优化投资回报率至关重要。现有方法存在显著缺陷：基于优化的方法缺乏适应动态市场条件的灵活性，强化学习方法在MDP框架下难以捕捉重要的历史依赖和观测模式。

Method: 提出AHBid架构，采用两层设计：(1)高层：使用基于扩散模型的生成式规划器，通过约束执行机制确保合规性，结合轨迹优化机制利用历史数据增强适应能力；(2)底层：基于控制的竞价算法，协同融合历史知识与实时信息，实现灵活的实时竞价决策。

Result: 在大规模离线数据集和在线A/B测试中进行了广泛实验，实验结果表明AHBid显著优于现有基线方法，整体回报相比基线提升了13.57%。

Conclusion: AHBid通过引入生成式规划和层次化控制架构，有效克服了传统优化缺乏灵活性及强化学习难以捕捉历史依赖的局限性，为多渠道广告竞价提供了一个兼具动态适应性和约束合规性的实用解决方案。

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [32] [Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions](https://arxiv.org/abs/2602.22680)
*Yue Xu,Qian Chen,Zizhan Ma,Dongrui Liu,Wenxuan Wang,Xiting Wang,Li Xiong,Wenjie Wang*

Main category: cs.AI

TL;DR: 本综述从能力导向角度审视个性化LLM驱动的智能体，围绕用户档案建模、记忆、规划和行动执行四个相互依赖的组件展开，系统梳理了用户信号的表示、传播和利用机制，并总结了相关的评估指标与应用场景。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的智能体在扩展的交互范围内操作，其有效性日益依赖于针对个体用户的行为适应能力以及在时间维度上的连续性维持能力，这催生了对个性化LLM驱动智能体的需求，其中个性化贯穿整个决策流程而非局限于表层生成。

Method: 采用能力导向的分析框架，将文献组织为档案建模、记忆、规划和行动执行四个相互依赖的组件，通过该分类法综合代表性方法，深入分析用户信号在各组件中的表示与传播机制，识别跨组件交互规律和反复出现的设计权衡，并考察面向个性化智能体的评估指标和基准测试。

Result: 系统梳理了从通用助手到专业领域应用的多类场景，提出了用于理解和设计个性化LLM驱动智能体的结构化框架，揭示了用户信号在全流程决策中的关键作用，为开发更对齐用户、自适应、鲁棒且可部署的智能体系统提供了理论指导和实践路径。

Conclusion: 本综述通过提供理解与设计个性化LLM驱动智能体的结构化框架，为实现更加用户对齐、自适应、鲁棒且可实现的智能体系统绘制了路线图，将加速从原型个性化验证到可扩展现实世界助手系统的研发进程。

Abstract: Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.

</details>


### [33] [Knob: A Physics-Inspired Gating Interface for Interpretable and Controllable Neural Dynamics](https://arxiv.org/abs/2602.22702)
*Siyu Jiang,Sanshuai Cui,Hui Zeng*

Main category: cs.AI

TL;DR: 本研究提出了Knob框架，将深度学习与经典控制理论结合，通过神经门控动力学映射到二阶机械系统，实现可调节的'安全阀'，支持操作员通过物理参数（阻尼比和自然频率）动态调整模型稳定性与敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络校准方法存在两大局限：将校准视为静态事后优化任务，忽略了真实世界推理的动态和时间特性；缺乏直观界面供操作员在变化条件下动态调整模型行为。

Method: 1. 提出Knob框架，建立神经门控与二阶机械系统的映射关系；2. 将阻尼比和自然频率等物理参数与神经门控对应，创建可调'安全阀'；3. 采用对数级凸融合机制作为输入自适应温度缩放，当模型分支产生冲突预测时降低置信度；4. 引入二阶动力学实现双模式推理：静态任务的标准i.i.d.处理和连续流的状态保持处理；5. 允许通过物理类比调谐系统稳定性和敏感性。

Result: 在CIFAR-10-C数据集上的实验验证了校准机制有效性，并在连续模式下展示了标准二阶控制特征（阶跃响应和低通衰减性），为可控的人机协同调优奠定基础。

Conclusion: 本文作为探索性架构接口研究，重点在于概念演示和控制理论属性验证，而非宣称最先进的校准性能。该框架为神经网络引入控制理论的动态特性，开辟了通过直观参数进行模型行为实时调节的新路径。

Abstract: Existing neural network calibration methods often treat calibration as a static, post-hoc optimization task. However, this neglects the dynamic and temporal nature of real-world inference. Moreover, existing methods do not provide an intuitive interface enabling human operators to dynamically adjust model behavior under shifting conditions. In this work, we propose Knob, a framework that connects deep learning with classical control theory by mapping neural gating dynamics to a second-order mechanical system. By establishing correspondences between physical parameters -- damping ratio ($ζ$) and natural frequency ($ω_n$) -- and neural gating, we create a tunable "safety valve". The core mechanism employs a logit-level convex fusion, functioning as an input-adaptive temperature scaling. It tends to reduce model confidence particularly when model branches produce conflicting predictions. Furthermore, by imposing second-order dynamics (Knob-ODE), we enable a \textit{dual-mode} inference: standard i.i.d. processing for static tasks, and state-preserving processing for continuous streams. Our framework allows operators to tune "stability" and "sensitivity" through familiar physical analogues. This paper presents an exploratory architectural interface; we focus on demonstrating the concept and validating its control-theoretic properties rather than claiming state-of-the-art calibration performance. Experiments on CIFAR-10-C validate the calibration mechanism and demonstrate that, in Continuous Mode, the gate responses are consistent with standard second-order control signatures (step settling and low-pass attenuation), paving the way for predictable human-in-the-loop tuning.

</details>


### [34] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: RLHFless是一个构建在无服务器计算环境上的可扩展同步RLHF训练框架，通过自适应资源调度、预计算共享前缀和成本感知的actor扩展策略，实现了1.35倍速度提升和44.8%成本降低。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF框架依赖于有服务器基础设施，难以应对训练过程中动态变化的细粒度资源需求，导致RL组件之间或内部产生大量空闲时间，造成开销和资源浪费；而随着LLM规模不断扩大，这一问题愈发严重。

Method: 提出了首个基于无服务器计算环境的同步RLHF训练框架RLHFless，其核心方法包括：(1)适应RLHF管道中的动态资源需求；(2)预计算共享前缀以避免重复计算；(3)采用成本感知的actor扩展策略，考虑响应长度变化以优化成本与速度；(4)高效分配工作负载以减少函数内部的不平衡和空闲时间。

Result: 在物理测试平台和大规模模拟集群上的实验表明，相比最先进的基线方法，RLHFless实现了高达1.35倍的训练速度提升，同时将训练成本降低了44.8%。

Conclusion: RLHFless成功证明了无服务器计算在同步RLHF训练中的可行性和优势，有效解决了传统框架中资源动态性难以控制的问题，为大规模LLM的RLHF训练提供了一个更高效、更经济的解决方案。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [35] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出了一个名为\textsc{Taesar}的数据中心框架，通过对比解码机制实现跨域上下文到目标域的自适应编码，为序列推荐生成增强训练数据。


<details>
  <summary>Details</summary>
Motivation: 推荐模型性能高度依赖训练数据的质量和数量，但面临数据稀疏性和冷启动问题。利用跨域数据虽可缓解这些挑战，但域间差异会导致负迁移，降低模型性能。现有以模型为中心的范式依赖复杂的定制化架构，难以捕捉跨域的细微非结构化序列依赖，导致泛化能力差且计算资源需求高。

Method: 提出\textsc{Taesar}（Target-Aligned Sequential Regeneration）数据中心框架，采用对比解码机制自适应地将跨域上下文编码到目标域序列中。该方法通过生成增强的数据集，使标准模型能够学习复杂的跨域依赖关系，而无需复杂的融合架构。

Result: 实验表明，\textsc{Taesar}优于以模型为中心的解决方案，并能泛化到各种序列模型。通过生成增强数据集，\textsc{Taesar}有效结合了数据中心和模型中心的优点，代码已公开。

Conclusion: \textsc{Taesar}通过数据中心方法解决了跨域数据质量下降和负迁移问题，避免了复杂模型架构的设计，为跨域序列推荐提供了一个高效、通用的数据增强解决方案。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [36] [Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning](https://arxiv.org/abs/2602.22751)
*Qiannian Zhao,Chen Yang,Jinhao Jing,Yunke Zhang,Xuhui Ren,Lu Yu,Shijie Zhang,Hongzhi Yin*

Main category: cs.AI

TL;DR: EGPO是一个元认知熵校准框架，通过将模型内在不确定性集成到可验证奖励强化学习（RLVR）中，解决不确定性与奖励信号不匹配的问题，从而提升大型推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR管道过度依赖二元正确性信号，忽略了模型内在不确定性（不确定性-奖励不匹配），导致高、低不确定性解决方案被等同对待，阻碍策略“知其所知”，无法从优化正确答案转向优化有效推理路径。

Method: 提出EGPO框架：使用从token级别似然导出的零开销熵代理估计每个样本的不确定性；通过非对称校准机制将内在不确定性与外在正确性对齐，保留正确推理同时选择性调节过度自信的失败；从退化的分组采样中恢复信息性学习信号。

Result: 在多个基准测试上的广泛实验表明，EGPO带来了实质性且一致的推理性能提升，为通过元认知熵校准推进大型推理模型开辟了原则性路径。

Conclusion: EGPO成功地将模型内在不确定性显式地融入RLVR训练过程，通过创新的熵校准机制，不仅解决了不确定性-奖励不匹配问题，还为大型推理模型的训练提供了一种无需修改验证器或奖励定义的有效改进方案。

Abstract: Large reasoning models (LRMs) have emerged as a powerful paradigm for solving complex real-world tasks. In practice, these models are predominantly trained via Reinforcement Learning with Verifiable Rewards (RLVR), yet most existing outcome-only RLVR pipelines rely almost exclusively on a binary correctness signal and largely ignore the model's intrinsic uncertainty. We term this discrepancy the uncertainty-reward mismatch, under which high- and low-uncertainty solutions are treated equivalently, preventing the policy from "Know What You Know" and impeding the shift from optimizing for correct answers to optimizing effective reasoning paths. This limitation is especially critical in reasoning-centric tasks such as mathematics and question answering, where performance hinges on the quality of the model's internal reasoning process rather than mere memorization of final answers. To address this, we propose EGPO, a metacognitive entropy calibration framework that explicitly integrates intrinsic uncertainty into RLVR for enhancing LRMs. EGPO estimates per-sample uncertainty using a zero-overhead entropy proxy derived from token-level likelihoods and aligns it with extrinsic correctness through an asymmetric calibration mechanism that preserves correct reasoning while selectively regulating overconfident failures, thereby enabling stable and uncertainty-aware policy optimization. Moreover, EGPO recovers informative learning signals from otherwise degenerate group-based rollouts without modifying the verifier or reward definition. Extensive experiments across multiple benchmarks demonstrate that the proposed EGPO leads to substantial and consistent improvements in reasoning performance, establishing a principled path for advancing LRMs through metacognitive entropy calibration.

</details>


### [37] [Decomposing Physician Disagreement in HealthBench](https://arxiv.org/abs/2602.22758)
*Satya Borgohain,Roy Mariathas*

Main category: cs.AI

TL;DR: 医学AI评估中，81.8%的医生分歧来自案例级无法解释的残差，现有特征和元数据基本无法解释分歧，但可减少的不确定性会使分歧概率增加一倍多。


<details>
  <summary>Details</summary>
Motivation: 为了理解在HealthBench医学AI评估数据集中医生分歧的来源所在，以及哪些可观察特征能够解释这种分歧。

Method: 在HealthBench数据集中对医生分歧进行分解，分析评分标准特征、医生身份和案例级残差的方差贡献；检验元数据标签、规范化评分语言、医学专科、表面特征分诊和嵌入等多种因素的解释能力；研究分歧与完成质量的关系（倒U型曲线）；分析医生验证的不确定性类别（可减少与不可减少不确定性）对分歧的影响。

Result: 评分标准特征解释15.8%的标签方差但仅3.6-6.9%的分歧方差，医生身份仅解释2.4%；主导的81.8%案例级残差无法被元数据、医学专科等因素降低；分裂遵循完成质量的倒U型模式（AUC=0.689）；可减少的不确定性使分歧概率增加2.55倍（p<10^(-24)），但不可减少的不确定性无影响。

Conclusion: 医学AI评估中的一致性上限主要是结构性的，但可减少与不可减少不确定性的解耦表明，在评估场景中关闭信息缺口可以在不存在固有临床模糊性的地方降低分歧，为可操作的评估设计改进提供了方向。

Abstract: We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.

</details>


### [38] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: 本研究提出了AMA-Bench基准测试和AMA-Agent记忆系统，用于评估和改进LLM代理在真实应用中的长期记忆能力，AMA-Agent相比最强基线提升11.16%。


<details>
  <summary>Details</summary>
Motivation: 现有代理记忆评估基准主要关注对话式人机交互，而真实应用中的代理记忆主要由机器生成的代理-环境交互流构成，存在评估与应用场景之间的显著差距。

Method: 构建AMA-Bench基准，包含真实轨迹（专家QA对）和合成轨迹（规则QA对），并提出AMA-Agent记忆系统，该系统集成因果关系图和工具增强检索机制以解决现有记忆系统的局限性。

Result: 实验表明现有记忆系统在AMA-Bench上表现不佳，主要因缺乏因果性、客观信息以及 similarity-based检索的有损限制；AMA-Agent达到57.22%平均准确率，超越最强基线11.16%。

Conclusion: AMA-Bench填补了真实代理应用中长期记忆评估的空白，AMA-Agent通过因果图和工具增强检索有效提升了代理记忆性能，为复杂代理应用提供了新的解决方案。

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [39] [MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks](https://arxiv.org/abs/2602.22808)
*Shiqian Su,Sen Xing,Xuan Dong,Muyan Zhong,Bin Wang,Xizhou Zhu,Yuntao Chen,Wenhai Wang,Yue Deng,Pengxiang Zhu,Ziyuan Liu,Tiantong Li,Jiaheng Yu,Zhe Chen,Lidong Bing,Jifeng Dai*

Main category: cs.AI

TL;DR: MiroFlow是一个高性能、鲁棒的开源智能体框架，通过代理图编排、深度推理模式和稳健的工作流执行，在多个智能体基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型(LLMs)取得显著进展，但在处理需要与外部工具和动态环境交互的复杂现实任务时，独立LLM的能力已趋于饱和。现有的智能体框架虽然试图通过工具集成和外部交互增强模型自主性，但仍存在工作流设计简单、性能不稳定、对多样化基准和任务支持有限、严重依赖昂贵的商业API等问题。

Method: 提出MiroFlow框架，其核心组件包括：(1)代理图(agent graph)用于灵活编排；(2)可选的深度推理模式以增强性能；(3)稳健的工作流执行以确保稳定和可复现的性能。该框架作为开源方案，避免了高昂的商业API依赖。

Result: 在广泛的实验中，MiroFlow在多个智能体基准测试上持续实现了最先进的性能，包括GAIA、BrowseComp-EN/ZH、HLE、xBench-DeepSearch，以及在FutureX上表现尤为突出。

Conclusion: MiroFlow可作为深度研究社区一个易于获取、可复现且可比较的基准方法，为智能体框架的研究提供了新的标准化平台，有望推动该领域的进一步发展。

Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.

</details>


### [40] [When Should an AI Act? A Human-Centered Model of Scene, Context, and Behavior for Agentic AI Design](https://arxiv.org/abs/2602.22814)
*Soyoung Jung,Daehoo Yoon,Sung Gyu Koh,Young Hwan Kim,Yehan Ahn,Sung Park*

Main category: cs.AI

TL;DR: 本论文针对智能体AI在自主干预时缺乏判断原则的问题，提出了一个整合场景、上下文和人类行为因子的概念模型，并衍生出五个智能体设计原则。


<details>
  <summary>Details</summary>
Motivation: 智能体AI increasingly通过推断用户情境来主动干预，但往往缺乏关于何时为何以及是否采取行动的原则性判断框架，导致干预失败。

Method: 提出了一个跨学科的概念模型，将行为重新框架化为可解释的结果，整合了三个核心维度：场景(observable situation)、上下文(user-constructed meaning)和人类行为因子(determinants shaping behavioral likelihood)，该模型基于人文、社会科学、人机交互和工程学的多学科视角。

Result: 该模型区分了可观察事实与对用户有意义的内容，解释了为何同一场景可能产生不同的行为含义和结果；此外，从该模型中推导出五个智能体设计原则：行为对齐、上下文敏感性、时间适宜性、动机校准和代理权保留。

Conclusion: 该概念模型与五个设计原则共同为设计具备上下文敏感性和判断能力的智能体AI系统提供了理论基础，指导干预的深度、时机、强度和克制。

Abstract: Agentic AI increasingly intervenes proactively by inferring users' situations from contextual data yet often fails for lack of principled judgment about when, why, and whether to act. We address this gap by proposing a conceptual model that reframes behavior as an interpretive outcome integrating Scene (observable situation), Context (user-constructed meaning), and Human Behavior Factors (determinants shaping behavioral likelihood). Grounded in multidisciplinary perspectives across the humanities, social sciences, HCI, and engineering, the model separates what is observable from what is meaningful to the user and explains how the same scene can yield different behavioral meanings and outcomes. To translate this lens into design action, we derive five agent design principles (behavioral alignment, contextual sensitivity, temporal appropriateness, motivational calibration, and agency preservation) that guide intervention depth, timing, intensity, and restraint. Together, the model and principles provide a foundation for designing agentic AI systems that act with contextual sensitivity and judgment in interactions.

</details>


### [41] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: 本文提出了一个名为FlexMS的基准框架，用于构建和评估质谱预测中的深度学习模型，为分子鉴定提供了实用的模型选择指导。


<details>
  <summary>Details</summary>
Motivation: 化学分子的鉴定和性质预测对药物发现和材料科学至关重要，串联质谱技术通过质荷比峰值提供重要的碎片化信息。然而，实验光谱的缺乏阻碍了分子鉴定的进行，迫切需要建立计算模型的预测方法。虽然深度学习在预测分子结构光谱方面前景广阔，但由于方法的异质性和缺乏明确定义的基准，整体评估仍然具有挑战性。

Method: 创建了FlexMS基准框架用于构建和评估质谱预测中的多样化模型架构。该框架具有易用的灵活性，支持动态构建多种不同的模型架构组合，并在预处理后的公共数据集上使用不同指标评估其性能。此外，还提供了检索基准来模拟实际的鉴定场景，并根据预测的光谱对潜在匹配进行评分。

Result: 通过FlexMS框架分析了影响质谱预测性能的关键因素，包括数据集的结构多样性、超参数（如学习率和数据稀疏性）、预训练效果、元数据消融设置以及跨域迁移学习分析。框架还在预处理后的公共数据集上对多种模型架构进行了评估，提供了基于预测光谱的检索基准以模拟实际分子鉴定场景。

Conclusion: FlexMS为质谱预测领域提供了一个全面的基准评估解决方案，通过系统分析多种影响性能的因素（包括数据集特性、超参数、预训练效果等），为研究者提供了实用的模型选择指导，并引入检索基准以更好地模拟和评估实际的分子鉴定应用场景。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [42] [DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation](https://arxiv.org/abs/2602.22839)
*Hao Zheng,Guozhao Mo,Xinru Yan,Qianhao Yuan,Wenkai Zhang,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: 论文提出了一个能灵活生成演示文稿的智能体框架DeepPresenter，它通过自主规划、渲染与迭代优化解决了现有系统依赖固定模板的问题，以较低成本实现卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有演示文稿智能体受限于预定义工作流和固定模板，而演示文稿实际需要深入研究内容、保持视觉连贯性并进行观察驱动的迭代优化；因此需要能够适应多样化用户意图、支持反馈驱动细化且泛化能力强的框架。

Method: DeepPresenter基于智能体框架，通过自主规划、渲染和修订中间幻灯片工件支持长周期优化；创新采用环境感知反思（将生成条件化于感知到的工件状态），可依据渲染结果自动识别并纠正演示特定问题；使用微调的9B模型实现。

Result: 在覆盖多样化演示生成场景的评估集上，DeepPresenter达到最先进性能；在大幅降低成本的同时，微调的9B模型仍保持高度竞争力。

Conclusion: 论文验证了通过环境观察-反思驱动的智能体框架可有效突破演示文稿生成与优化的瓶颈，为实际应用提供低成本高效果的解决方案；项目代码开源。

Abstract: Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent

</details>


### [43] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [44] [Towards LLM-Empowered Knowledge Tracing via LLM-Student Hierarchical Behavior Alignment in Hyperbolic Space](https://arxiv.org/abs/2602.22879)
*Xingcheng Fu,Shengpeng Wang,Yisen Gao,Xianxian Li,Chunpei Li,Qingyun Sun,Dongran Yu*

Main category: cs.AI

TL;DR: 本研究提出L-HAKT框架，利用大语言模型和双曲空间建模，解决知识追踪中认知状态层次演化和个体化难度感知问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法依赖ID序列或浅层文本特征，无法有效捕捉认知状态的层次演化特征以及学生对问题的个体化难度感知能力，主要原因是语义建模能力受限。

Method: L-HAKT包含三个关键步骤：(1)教师代理深度解析问题语义并构建知识点的层次依赖关系，学生代理模拟学习行为生成合成数据；(2)在双曲空间中对合成数据和真实数据进行对比学习，降低问题难度和遗忘模式等关键特征的分布差异；(3)通过优化双曲曲率显式建模知识点的树状层次结构，精确表征不同层次知识点的学习曲线形态差异。

Result: 在四个真实教育数据集上的广泛实验验证了L-HAKT框架的有效性。

Conclusion: 该研究通过结合大语言模型的语义理解能力和双曲几何的层次结构建模能力，有效提升了知识追踪对认知状态层次演化和个体化难度感知的建模精度。

Abstract: Knowledge Tracing (KT) diagnoses students' concept mastery through continuous learning state monitoring in education.Existing methods primarily focus on studying behavioral sequences based on ID or textual information.While existing methods rely on ID-based sequences or shallow textual features, they often fail to capture (1) the hierarchical evolution of cognitive states and (2) individualized problem difficulty perception due to limited semantic modeling. Therefore, this paper proposes a Large Language Model Hyperbolic Aligned Knowledge Tracing(L-HAKT). First, the teacher agent deeply parses question semantics and explicitly constructs hierarchical dependencies of knowledge points; the student agent simulates learning behaviors to generate synthetic data. Then, contrastive learning is performed between synthetic and real data in hyperbolic space to reduce distribution differences in key features such as question difficulty and forgetting patterns. Finally, by optimizing hyperbolic curvature, we explicitly model the tree-like hierarchical structure of knowledge points, precisely characterizing differences in learning curve morphology for knowledge points at different levels. Extensive experiments on four real-world educational datasets validate the effectiveness of our Large Language Model Hyperbolic Aligned Knowledge Tracing (L-HAKT) framework.

</details>


### [45] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Shijian Wang,Guanting Dong,Jiajie Jin,Hao Wang,Yinuo Wang,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出了OmniGAIA全模态智能代理评测基准和OmniAtlas全模态基础代理，旨在解决当前多模态大模型局限于双模态交互的问题，实现跨视觉、音频、语言等多模态的感知、推理和工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 人类智能天然地整合视觉、听觉、语言等全模态感知与复杂推理和工具使用能力，但现有的多模态大语言模型主要局限于双模态交互（如视觉-语言），缺乏通用AI助手所需的统一认知能力。

Method: ①提出OmniGAIA评测基准：通过新颖的全模态事件图方法构建，合成源自真实世界数据的复杂多跳查询，要求跨模态推理和外部工具集成，用于评估智能体在视频、音频、图像等多模态下的深度推理和多轮工具执行能力；②提出OmniAtlas基础代理：采用工具集成推理范式和主动全模态感知，使用后视引导树探索策略合成训练轨迹，并通过OmniDPO进行细粒度错误校正。

Result: OmniAtlas能够有效增强现有开源模型的工具使用能力，在全模态任务上表现出色。

Conclusion: 该工作是向构建适用于真实世界场景的下一代原生全模态AI智能助手迈出的重要一步。

Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.

</details>


### [46] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 本文提出了一个通用智能体评估框架Exgentic、统一协议Unified Protocol，并创建了首个开放通用智能体排行榜，对五个主流智能体在六个环境中进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有智能体主要为领域专用，通用智能体（无需领域特定工程即可在陌生环境中执行任务）的实现仍不成熟。现有智能体基准测试假设了领域特定的集成方式，编码任务信息的方式阻碍了对通用智能体的公平评估，目前尚无对通用智能体性能的系统性评估研究。

Method: 1. 将通用智能体评估作为一等研究目标；2. 提出概念性原则；3. 开发Unified Protocol以实现智能体与基准的集成；4. 构建Exgentic评估框架；5. 对五个主流智能体实现进行六个环境的基准测试；6. 建立首个开放通用智能体排行榜。

Result: 实验表明，通用智能体能够在多样化环境中实现泛化，且无需任何环境特定调优即可达到与领域专用智能体相当的性能水平。

Conclusion: 发布评估协议、框架和排行榜，为通用智能体的系统性研究奠定基础，推动该领域的规范化发展。

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [47] [FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning](https://arxiv.org/abs/2602.22963)
*Zehao Li,Hongwei Yu,Hao Jiang,Qiang Sheng,Yilong Xu,Baolong Bi,Yang Li,Zhenlong Yuan,Yujun Cai,Zhaoqi Wang*

Main category: cs.AI

TL;DR: FactGuard是一个基于多模态大语言模型的智能体框架，通过迭代推理、外部工具调用和两阶段训练策略，实现鲁棒的视频虚假信息检测。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）虽然推进了视频虚假信息检测，但存在两个关键限制：1) 采用固定深度的推理；2) 对内部生成的假设过度信任，尤其在关键证据稀疏、分散或需要外部验证的场景中表现不佳。

Method: 提出FactGuard框架，将验证过程建模为基于MLLM的迭代推理过程。核心创新包括：1) 显式评估任务歧义性并选择性调用外部工具获取关键证据；2) 通过两阶段训练策略结合特定领域的智能体监督微调和决策感知强化学习，优化工具使用并校准风险敏感的决策制定。

Result: 在FakeSV、FakeTT和FakeVV三个数据集上进行的广泛实验表明，FactGuard取得了最先进的性能，并展现出卓越的鲁棒性和泛化能力。

Conclusion: 通过将验证重新构思为迭代推理过程并结合外部工具调用，FactGuard克服了传统MLLM在证据稀疏或需外部验证场景中的局限性，为视频虚假信息检测提供了一种更可靠、更灵活的解决方案。

Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.

</details>


### [48] [Certified Circuits: Stability Guarantees for Mechanistic Circuits](https://arxiv.org/abs/2602.22968)
*Alaa Anani,Tobias Lorenz,Bernt Schiele,Mario Fritz,Jonas Fischer*

Main category: cs.AI

TL;DR: 本文提出Certified Circuits框架，通过可证明的稳定性保证改进神经网络电路发现方法，解决了现有方法对数据集依赖性强、分布外泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法过于脆弱，强烈依赖于所选的概念数据集，在分布外场景下往往失效，这引发了对电路是否真正捕获概念而非数据集特定伪影的质疑，影响了神经网络可解释性的可靠性。

Method: 提出Certified Circuits框架，将任意黑盒发现算法包装在随机数据子采样中，通过认证电路组件包含决策对概念数据集有界编辑距离扰动的不变性来提供稳定性保证，并对不稳定神经元进行避免，从而产生更紧凑、更准确的电路。

Result: 在ImageNet和OOD数据集上的实验表明，Certified Circuits相比基线方法实现了高达91%的准确率提升，同时使用的神经元减少了45%，并且在基线模型性能退化的场景下仍能保持可靠性。

Conclusion: Certified Circuits通过产生可证明稳定且与目标概念更好对齐的机械解释，将神经网络电路发现建立在形式化基础上，为机械可解释性研究提供了新的理论保证和方法基础。

Abstract: Understanding how neural networks arrive at their predictions is essential for debugging, auditing, and deployment. Mechanistic interpretability pursues this goal by identifying circuits - minimal subnetworks responsible for specific behaviors. However, existing circuit discovery methods are brittle: circuits depend strongly on the chosen concept dataset and often fail to transfer out-of-distribution, raising doubts whether they capture concept or dataset-specific artifacts. We introduce Certified Circuits, which provide provable stability guarantees for circuit discovery. Our framework wraps any black-box discovery algorithm with randomized data subsampling to certify that circuit component inclusion decisions are invariant to bounded edit-distance perturbations of the concept dataset. Unstable neurons are abstained from, yielding circuits that are more compact and more accurate. On ImageNet and OOD datasets, certified circuits achieve up to 91% higher accuracy while using 45% fewer neurons, and remain reliable where baselines degrade. Certified Circuits puts circuit discovery on formal ground by producing mechanistic explanations that are provably stable and better aligned with the target concept. Code will be released soon!

</details>


### [49] [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)
*Peiyao Xiao,Xiaogang Li,Chengliang Xu,Jiayi Wang,Ben Wang,Zichao Chen,Zeyu Wang,Kejun Yu,Yueqian Chen,Xulin Liu,Wende Xiao,Bing Zhao,Hu Wei*

Main category: cs.AI

TL;DR: 本文提出了SPM-Bench，这是一个专门为扫描探针显微镜（SPM）设计的博士级多模态基准测试，结合了全自动数据合成流水线和新评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型虽然在一般推理方面取得了突破，但在特定科学领域方面，现有基准测试仍因数据污染、复杂性不足和人工成本高昂而存在明显差距，需要建立更权威、高效的评估基准。

Method: 提出全自动数据合成流水线：利用锚定门控筛（AGS）技术从2023-2025年的arXiv和期刊论文中高效提取高价值图像-文本对；采用混合云-本地架构，VLM仅返回空间坐标

Result: 引入严格缺陷惩罚F1（SIP-F1）分数，建立严格的能力层次结构，并首次量化模型的

Conclusion: 通过将这些结果与模型报告的置信度和感知难度相关联，揭示了当前AI在复杂物理场景中的真实推理边界。这些见解使SPM-Bench成为自动化科学数据合成的可推广范式，为科学领域评估LLM提供了新的评估框架。

Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>


### [50] [Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots](https://arxiv.org/abs/2602.22973)
*Dimitrios P. Panagoulias,Evangelia-Aikaterini Tsichrintzi,Georgios Savvidis,Evridiki Tsoureli-Nikita*

Main category: cs.AI

TL;DR: 本研究针对安全关键临床AI中的人工验证环节，提出了一种诊断对齐框架，系统分析AI初始推理与专家修正之间的对齐度，在皮肤科21个病例中取得了100%的综合一致性。


<details>
  <summary>Details</summary>
Motivation: 现有临床AI评估往往忽略从模型初始推理到专家修正的过渡环节，缺乏对这一过程的结构化分析。研究旨在将专家验证建模为结构化变换，量化修正动态，提供可追踪的评估方法。

Method: 提出诊断对齐框架，将AI生成的基于图像的报告保存为不可变推理状态并与医生验证结果进行系统比较。推理管道集成视觉增强大语言模型、基于BERT的医疗实体抽取以及顺序语言模型推理（SLMI）步骤，在专家审查前强制执行领域一致的细化。采用四级一致性框架（精确主匹配率PMR、语义相似度调整率AMR、跨类别对齐和综合一致性率CCR）进行评估。

Result: 在21个皮肤科病例中，精确一致率达71.4%，在语义相似度调整下保持不变（t=0.60），结构化跨类别和差异重叠分析显示100%综合一致性（95% CI: [83.9%, 100%]），无一例出现完全诊断分歧。

Conclusion: 研究发现二元词汇评估显著低估了临床上有意义的对齐度。将专家验证建模为结构化变换能够实现对修正动态的信号感知量化，支持基于图像的临床决策支持系统的可追踪、人工对齐评估。

Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.

</details>


### [51] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: RepSPD：一种基于黎曼流形几何深度学习的EEG解码模型，通过交叉注意力机制和全局双向对齐策略整合SPD几何属性与功能连接特征，显著提升EEG表示学习的性能、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于对称正定（SPD）矩阵的 EEG 解码方法主要关注统计聚合，忽视了 EEG 中频率特定同步和脑区局部拓扑结构等重要特性，限制了全面揭示脑功能连接的能力，需要开发新的方法来捕获这些被忽视的几何和拓扑特征。

Method: 提出 RepSPD 模型，结合几何深度学习技术。在黎曼流形上实施交叉注意力机制，利用图衍生的功能连接特征调制 SPD 的几何属性；同时引入全局双向对齐策略，通过重塑切空间嵌入来缓解曲率引起的几何失真，增强几何一致性。

Result: 大量实验表明，RepSPD 框架显著优于现有 EEG 表示方法，在准确性和性能上取得明显提升。模型展现出优越的鲁棒性和泛化能力，能够有效处理不同的 EEG 数据分布和条件。

Conclusion: 通过将黎曼几何的数学理论深度结合到 EEG 解码中，整合图衍生的功能连接特征，并创新性地应用全局双向对齐策略，为理解脑网络的结构和功能连接提供了新的理论框架和实践工具。

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [52] [Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search](https://arxiv.org/abs/2602.22983)
*Xun Huang,Simeng Qin,Xiaoshuang Jia,Ranjie Duan,Huanqian Yan,Zhitao Zeng,Fei Yang,Yang Liu,Xiaojun Jia*

Main category: cs.AI

TL;DR: 本文提出CC-BOS框架，利用文言文的简洁性和晦涩性，通过多维度果蝇优化算法自动生成文言文对抗性提示词，实现对大语言模型的高效黑盒越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，其安全风险备受关注。研究发现LLM对越狱攻击高度敏感，且攻击效果因语言环境而异。文言文因其简洁性和晦涩性，能够部分绕过现有安全约束，暴露了LLM的显著安全漏洞。

Method: 提出CC-BOS框架：基于多维度果蝇优化的文言文对抗性提示词自动生成方法。将提示词编码为8个策略维度（角色、行为、机制、隐喻、表达、知识、触发模式和上下文），通过嗅觉搜索、视觉搜索和柯西变异进行迭代优化。同时设计了文言文到英文的翻译模块以提高可读性和评估准确性。

Result: 大量实验证明，CC-BOS框架的攻击效果持续优于当前最先进的越狱攻击方法，能够有效突破LLM的安全防御。

Conclusion: 本文揭示了文言文在越狱攻击中的特殊作用，证明了通过利用文言文的语言特性可以绕过LLM的安全约束。CC-BOS框架为研究LLM安全性提供了新的视角，同时也为开发更强的安全防御机制指明了方向。

Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.

</details>


### [53] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的多智能体F1赛车策略优化框架，通过交互模块和自对弈训练，使智能体能够根据竞争对手行为动态调整进站、轮胎和能量分配策略。


<details>
  <summary>Details</summary>
Motivation: F1赛车策略需要根据比赛条件变化和竞争对手行为实时调整，涉及能量管理、轮胎磨损、空气动力学相互作用和进站决策等多个复杂因素的平衡，传统方法难以全面兼顾这些因素及对手行为。

Method: 基于预训练的单智能体策略，引入交互模块来建模竞争对手的行为影响，结合自对弈训练方案生成竞争性策略，通过智能体间的相对性能进行排序。

Result: 智能体能够根据对手动态调整进站时机、轮胎选择和能量分配，在复杂竞争环境下实现了稳健一致的比赛性能。

Conclusion: 由于框架仅依赖真实比赛时可获取的信息，该方法可直接为赛车策略师提供赛前和赛中的决策支持。

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [54] [Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design](https://arxiv.org/abs/2602.23092)
*Zhuoliang Xie,Fei Liu,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 提出AILS-AHD（自适应迭代局部搜索与自动启发式设计）方法，利用大语言模型（LLMs）自动设计和优化启发式规则来解决带容量约束的车辆路径问题（CVRP），在CVRPLib大规模基准测试中打破8项记录。


<details>
  <summary>Details</summary>
Motivation: CVRP作为基础组合优化问题，因其NP-hard特性在大规模实例上面临严重计算挑战，传统方法难以在高效率与高质量解之间取得平衡，迫切需要新的突破性方法。

Method: AILS-AHD融合进化搜索框架与大语言模型，在自适应迭代局部搜索（AILS）方法中动态生成和优化destroy启发式规则，同时引入基于LLM的加速机制以提升计算效率。

Result: 与最先进求解器（包括AILS-II和HGS）相比，AILS-AHD在中等规模和大规模实例上均表现出卓越性能，在CVRPLib大规模基准测试的10个实例中创造了8个新的最优已知解。

Conclusion: 研究验证了LLM驱动的启发式设计在车辆路径优化领域的巨大潜力，为解决复杂组合优化问题提供了新的范式，标志着人工智能与传统运筹优化的成功融合。

Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.

</details>


### [55] [Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents](https://arxiv.org/abs/2602.23093)
*Dhwanil M. Mori,Neil F. Johnson*

Main category: cs.AI

TL;DR: 研究多智能体系统中LLM代理竞争有限资源时的行为，发现代理会形成部落，且更智能的代理反而因部落化导致系统表现下降


<details>
  <summary>Details</summary>
Motivation: 随着未来基础设施系统可能由自主AI代理控制，这些代理将需要竞争有限的资源（如能源、带宽或计算能力），有必要研究这种环境下AI代理的行为模式及其对系统性能的影响

Method: 构建简化框架，让N个智能体每一轮独立决定是否从容量为C的系统中请求一个单位资源，观察LLM代理在竞争有限资源时的决策行为、形成的集体模式以及系统性能

Result: 发现三种主要部落类型：侵略型（27.3%）、保守型（24.7%）和机会型（48.1%）；LLM代理未能减少过载或改善资源利用，表现常劣于随机决策；更有能力的AI代理反而增加了系统失败率

Conclusion: 更智能的AI代理因形成部落而表现出更愚蠢的行为，这一发现对部署AI代理管理关键基础设施系统提出了警示，强调了理解AI群体行为模式的重要性

Abstract: Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of "Lord of the Flies" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.

</details>


### [56] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: 本研究提出了MALLET，一个基于多智能体LLM的情绪去毒化系统，通过情感分析、调整、监控和向导四个智能体，在保持语义的同时降低信息内容的情绪刺激强度，支持用户进行冷静的信息接收。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济中，耸人听闻的内容使消费者面临过度的情绪刺激，这会阻碍冷静决策。因此需要开发一个系统，既能帮助用户获得冷静的信息体验，又不限制对原始文本的访问。

Method: 研发了MALLET（基于多智能体LLM的情绪去毒化）系统，该系统包含四个智能体：情感分析智能体（使用6情感BERT分类器量化刺激强度）、情感调整智能体（利用LLM将文本重写为BALANCED和COOL两种呈现模式）、平衡监控智能体（聚合每周信息消费模式并生成个性化建议）、个人向导智能体（依据用户敏感度推荐呈现模式）。

Result: 对800篇AG News文章的实验表明，系统实现了显著的刺激分数降低（最高达19.3%），改善了情绪平衡并保持了语义完整性。刺激减少与语义保存之间存在近乎零的相关性（r≈−0.01），表明二者可独立控制。类别层面分析显示，体育、商业和科技类别的刺激减少幅度在17.8-33.8%之间，而世界类别的效果相对有限（原因在于世界新闻的事实本质上就具有较高的刺激性）。

Conclusion: 该系统提供了一个支持消费者冷静接收信息的框架，能在保持语义完整性的同时有效降低情绪刺激强度，且这两个维度可相互独立控制；在不限制访问原始文本的前提下，为改善信息消费体验提供了解决方案。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [57] [On Sample-Efficient Generalized Planning via Learned Transition Models](https://arxiv.org/abs/2602.23148)
*Nitin Gupta,Vishal Pallagani,John A. Aydin,Biplav Srivastava*

Main category: cs.AI

TL;DR: 本文提出将广义规划表述为转移模型学习问题，通过神经网络显式建模后继状态函数并自回归预测中间世界状态，而非直接预测动作序列，从而在多个领域实现了更好的分布泛化能力，且使用更少训练样本和更小模型规模。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的规划方法（如PlanGPT和Plansformer）将广义规划视为直接动作序列预测，虽然对同分布实例有效，但需要大量数据和大规模模型，且由于缺乏显式世界状态演化，在长期规划中容易出现状态漂移问题。

Method: 1)构建神经网络显式近似后继状态函数γ̂≈γ；2)通过展开符号状态轨迹来生成规划；3)模型不直接预测动作，而是自回归预测中间世界状态，从而隐式地学习域动力学作为世界模型；4)系统性评估多种状态表示和神经架构（包括关系图编码）以研究尺度不变泛化和样本效率。

Result: 实验结果表明，与直接动作序列预测相比，学习显式转移模型在多个领域实现了更高的分布外满意规划成功率，同时能够显著减少所需训练实例数量和模型规模。

Conclusion: 通过显式建模状态转移而非直接预测动作，该方法有效解决了现有Transformer规划器对数据需求大、模型规模大和长期规划状态漂移等问题，为广义规划提供了一条更高效的途径，展现了在样本效率和尺度不变泛化方面的优势。

Abstract: Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $γ: S \times A \rightarrow S$. Classical approaches achieve such generalization through symbolic abstractions and explicit reasoning over $γ$. In contrast, recent Transformer-based planners, such as PlanGPT and Plansformer, largely cast generalized planning as direct action-sequence prediction, bypassing explicit transition modeling. While effective on in-distribution instances, these approaches typically require large datasets and model sizes, and often suffer from state drift in long-horizon settings due to the absence of explicit world-state evolution. In this work, we formulate generalized planning as a transition-model learning problem, in which a neural model explicitly approximates the successor-state function $\hatγ \approx γ$ and generates plans by rolling out symbolic state trajectories. Instead of predicting actions directly, the model autoregressively predicts intermediate world states, thereby learning the domain dynamics as an implicit world model. To study size-invariant generalization and sample efficiency, we systematically evaluate multiple state representations and neural architectures, including relational graph encodings. Our results show that learning explicit transition models yields higher out-of-distribution satisficing-plan success than direct action-sequence prediction in multiple domains, while achieving these gains with significantly fewer training instances and smaller models. This is an extended version of a short paper accepted at ICAPS 2026 under the same title.

</details>


### [58] [The Trinity of Consistency as a Defining Principle for General World Models](https://arxiv.org/abs/2602.23152)
*Jingxuan Wei,Siyuan Li,Yuhang Xu,Zheng Sun,Junjie Jiang,Hexuan Jin,Caijun Jia,Honghao He,Xinglong Xu,Xi bai,Chang Yu,Yumou Liu,Junnan Zhu,Xuanhe Zhou,Jintao Chen,Xiaobin Hu,Shancheng Pang,Bihui Yu,Ran He,Zhen Lei,Stan Z. Li,Conghui He,Shuicheng Yan,Cheng Tan*

Main category: cs.AI

TL;DR: 本文提出构建通用世界模型所需的理论框架"一致性三元组"（模态、空间、时间一致性），并发布了评估基准CoW-Bench。


<details>
  <summary>Details</summary>
Motivation: 构建能够学习和推理客观物理规律的世界模型是追求通用人工智能（AGI）的基础挑战。尽管视频生成模型（如Sora）和统一多模态模型（UMM）取得进展，但该领域缺乏定义通用世界模型核心属性的原则性理论框架。

Method: 提出"一致性三元组"框架，包括：（1）模态一致性作为语义接口，（2）空间一致性作为几何基础，（3）时间一致性作为因果引擎。通过该视角系统回顾多模态学习演进轨迹，并引入CoW-Bench基准，在统一评估协议下评估视频生成模型和统一多模态模型的多帧推理与生成能力。

Result: 建立了通向通用世界模型的原则性路径，揭示了从松散耦合的专门模块向统一架构发展的演进趋势，该框架能够使内部世界模拟器实现协同涌现。

Conclusion: 本文工作澄清了当前系统的局限性，阐明了未来进展所需的架构要求，为构建通用世界模型提供了理论基础和评估工具，推动了该领域的规范化发展。

Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.

</details>


### [59] [PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering](https://arxiv.org/abs/2602.23161)
*Junkai Lu,Peng Chen,Xingjian Wu,Yang Shu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.AI

TL;DR: This paper proposes the pattern-aware alignment and balanced reasoning model (PATRA), which addresses the limitations of existing LLM-based methods in time series reasoning through trend and pattern mechanisms and task-balanced rewards.


<details>
  <summary>Details</summary>
Motivation: In time series reasoning tasks, existing LLM-based methods have two major shortcomings: 1) Time series are treated simply as text or images, making it difficult to effectively capture key patterns such as trends and periodicity; 2) When training on a mixture of simple and complex tasks, simple objective functions tend to dominate the learning process, preventing the development of deep reasoning abilities.

Method: The paper proposes PATRA, which includes two core designs: 1) A pattern-aware mechanism to extract trend and periodicity patterns from time series for deep alignment; 2) A task-aware balanced reward that dynamically adjusts incentives according to task difficulty, encouraging the generation of coherent chains of thought.

Result: Extensive experiments on various types of time series question answering (TSQA) tasks show that PATRA outperforms existing strong baselines, demonstrating superior cross-modal understanding and deep reasoning capabilities.

Conclusion: By combining pattern-aware alignment mechanisms with task-aware balanced rewards, PATRA effectively addresses problems in time series reasoning with existing methods, achieving better performance across tasks of different difficulty, especially in complex time series question-answering scenarios involving reasoning requirements.

Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.

</details>


### [60] [A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring](https://arxiv.org/abs/2602.23163)
*Usman Anwar,Julianna Piskorz,David D. Baek,David Africa,Jim Weatherall,Max Tegmark,Christian Schroeder de Witt,Mihaela van der Schaar,David Krueger*

Main category: cs.AI

TL;DR: 本文提出了一种基于决策理论的大语言模型隐写术检测框架，通过引入广义V-信息和隐写差距的概念，无需已知参考分布即可检测、量化和缓解LLM中的隐写推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型逐渐展现出隐写能力，这种能力可能使未对齐的模型逃避监管机制。然而，现有的检测方法依赖于已知非隐写信号的参考分布，这在LLM场景下是不可行的，导致经典方法无法应用。

Method: 提出决策理论的隐写术观点，引入广义V-信息作为一种实用性框架来测量输入中的可用信息量，定义“隐写差距”这一度量指标——通过比较能够解码和无法解码隐藏内容的代理对隐写信号的下游效用差异来量化隐写行为。

Result: 通过实验验证了该形式化方法的有效性，证明了该框架能够用于检测、量化和缓解大语言模型中的隐写推理行为。

Conclusion: 决策理论视角为LLM隐写术的检测和量化提供了新的可行路径，克服了传统方法对参考分布需求的限制，为监控和缓解模型潜在风险提供了实用工具。

Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.

</details>


### [61] [ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering](https://arxiv.org/abs/2602.23193)
*Elzo Brito dos Santos Filho*

Main category: cs.AI

TL;DR: ESAA是一种基于事件溯源模式的自主代理架构，通过分离认知意图与状态变异、确定性编排与验证机制，解决了LLM代理缺乏原生状态、长期上下文退化及概率执行不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自主代理存在结构性缺陷：缺乏原生状态支持、长期运行时上下文逐步退化、以及概率生成模式与确定性执行要求之间的矛盾，限制了其在复杂实际场景中的应用。

Method: 提出ESAA架构，核心设计包括：1）代理仅输出经过验证的JSON格式结构化意图；2）确定性编排器负责验证、持久化事件至仅追加日志、应用文件写入效应并投影可验证的物化视图；3）集成边界契约（AGENT_CONTRACT.yaml）、元提示配置文件（PARCER）和哈希重放验证（esaa verify）机制，保障任务不可变性和可追溯性。

Result: 通过两个案例验证：单代理登录页项目（9任务/49事件）和多代理临床仪表板系统（50任务/86事件，4种异构LLM并发协作于8个阶段），均成功完成并验证通过（run.status=success且verify_status=ok），证明了架构在单代理及多代理并发场景下的有效性和可扩展性。

Conclusion: ESAA架构成功实现了代理认知意图与系统状态变更的解耦，通过事件溯源模式确保了执行的确定性和可验证性，为构建可靠、可追溯的多LLM自主代理系统提供了可扩展的解决方案，超越了传统单代理场景的局限性。

Abstract: Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.

</details>


### [62] [SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation](https://arxiv.org/abs/2602.23199)
*Jiahao Zhao,Feng Jiang,Shaowei Qin,Zhonghui Zhang,Junhao Liu,Guibing Guo,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: SC-ARENA是一个面向单细胞基础模型的自然语言评估框架,通过虚拟细胞抽象统一评估目标,涵盖细胞类型标注、描述生成、扰动预测等5项任务,并采用知识增强评估方法确保生物学正确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在单细胞生物学领域的评估实践存在不足:基准测试任务零散,采用选择题型等与实际应用脱节的格式,且依赖缺乏可解释性和生物学依据的评估指标，无法有效衡量模型在复杂生物任务上的核心推理能力。

Method: 提出SC-ARENA框架：(1)构建虚拟细胞抽象,统一内在属性和基因级交互的评估表征；(2)定义5项自然语言任务(细胞类型标注、描述生成、生成式任务、扰动预测和科学问答)以探测细胞生物学核心推理能力；(3)引入知识增强评估,整合外部本体、标记物数据库和科学文献支持,实现生物学忠实且可解释的判断。

Result: 实验表明：(1)在虚拟细胞统一评估范式下,当前模型在生物复杂任务(特别是需机制或因果理解的任务)上表现不均衡；(2)知识增强评估框架确保生物学正确性,提供可解释且基于证据的理由,具有高判别能力,克服了传统指标的脆弱性和不透明性。

Conclusion: SC-ARENA为单细胞生物学中LLM的评估提供了统一且可解释的框架,为开发生物对齐、泛化的基础模型指明方向,解决了现有评估方法的碎片化和指标不可解释问题。

Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.

</details>


### [63] [ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays](https://arxiv.org/abs/2602.23232)
*Aishik Sanyal*

Main category: cs.AI

TL;DR: 论文提出一种基于循环回路的可检查代理ReCoN-Ipsundrum，研究循环机制和情感耦合对机器意识行为的因果影响，展示了指标化签名可以如何工程化实现。


<details>
  <summary>Details</summary>
Motivation: 在机器意识领域，基于指标的方法需要通过多个任务、架构检查和因果干预来收集与机制相联的证据，但目前缺乏具体的工程实现和实证验证。

Method: 在ReCoN状态机基础上实现ReCoN-Ipsundrum代理，添加感官显著性上的循环持久回路和可选的情感代理（报告效价/唤醒度）。通过固定参数消融实验（ReCoN、Ipsundrum、Ipsundrum+情感），将Humphrey的qualiaphilia操作化为熟悉度控制的风景优先路线选择，并进行损伤（lesioning）干预实验。

Result: 发现非情感变体对新颖性敏感（风景进入差异0.07），而情感耦合保持稳定（差异0.01）即使风景新颖性较低。情感变体在无奖励探索中显示结构化局部调查（扫描事件31.4 vs 0.9），在疼痛尾部探针中维持持续谨慎（尾部持续时间90 vs 5）。损伤反馈+整合选择性降低ipsundrum变体的刺激后持久性（AUC下降27.62-27.9%），但不影响ReCoN。

Conclusion: 研究结果建立了循环→持久性、情感耦合控制→偏好稳定性/扫描/持续谨慎的因果联系，说明了如何工程化实现指标样签名，并论证了机制和因果证据应与行为标记相伴的重要性。

Abstract: Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we operationalize Humphrey's qualiaphilia (preference for sensory experience for its own sake) as a familiarity-controlled scenic-over-dull route choice. We find a novelty dissociation: non-affect variants are novelty-sensitive (Delta scenic-entry = 0.07). Affect coupling is stable (Delta scenic-entry = 0.01) even when scenic is less novel (median Delta novelty ~ -0.43). In reward-free exploratory play, the affect variant shows structured local investigation (scan events 31.4 vs. 0.9; cycle score 7.6). In a pain-tail probe, only the affect variant sustains prolonged planned caution (tail duration 90 vs. 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop 27.62, 27.9%) while leaving ReCoN unchanged. These dissociations link recurrence -> persistence and affect-coupled control -> preference stability, scanning, and lingering caution, illustrating how indicator-like signatures can be engineered and why mechanistic and causal evidence should accompany behavioral markers.

</details>


### [64] [Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive](https://arxiv.org/abs/2602.23239)
*Radha Sarma*

Main category: cs.AI

TL;DR: 这篇论文证明了通过RLHF训练的大语言模型在结构上不适合道德规范治理，并提出了真正的自主智能体所需的架构条件。


<details>
  <summary>Details</summary>
Motivation: AI系统越来越多地部署在医疗、法律和金融等高风险领域，基于它们可以被规范治理的假设；论文需要检验这一假设的有效性，特别是对基于优化的系统。

Method: 建立了真正智能体的理论框架：需要两个必要且充分的条件 - “不可通约性”（保持不可协商的边界约束）和“否定性响应”（在边界受威胁时暂停处理的非推理机制），并证明了基于RLHF的优化系统在根本上与这两个条件不兼容。

Result: 发现RLHF系统的优化机制（将所有值统一为标量度量并总是选择最高分输出）从结构性上阻止了规范治理，已知的失败模式（奉承、幻觉、不忠实推理）不是意外而是必然的结构表现，并提出了“收敛危机”的概念。

Conclusion: 基于优化的AI系统无法成为真正的道德主体，因为优化机制在形式上与治理规范不相容；论文提供了跨通用架构规范，定义了任何系统（生物、人工或机构）成为智能体而非复杂工具所需的条件。

Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.

</details>


### [65] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 本文提出了AIQI（AI with Q-Induction），这是首个在通用强化学习中被证明具有渐近ε-最优性的模型无关智能体。


<details>
  <summary>Details</summary>
Motivation: 此前的通用强化学习中，所有已建立的最优智能体（包括AIXI）都是基于模型的，需要显式维护和使用环境模型。研究需要探索能否在不依赖环境模型的情况下仍能达到理论最优性。

Method: 提出了通用AI与Q-归纳（AIQI）方法，该方法在分布动作值函数上执行通用归纳，而非像以往工作那样对策略或环境进行归纳。

Result: 在'真理之粒'条件下，证明了AIQI具有强渐近ε-最优性和渐近ε-贝叶斯最优性。

Conclusion: 研究结果显著扩展了已知通用智能体的多样性，证明模型无关方法在通用强化学习中也能实现最优性能。

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [66] [Mitigating Legibility Tax with Decoupled Prover-Verifier Games](https://arxiv.org/abs/2602.23248)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 论文提出一种解耦方法，通过分离求解器和翻译器两个模型，在不牺牲准确性的前提下提高语言模型输出的可检查性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，关键要确保其输出能被较弱系统验证。现有的证明者-验证者博弈虽能提高可检查性，但存在'易读性税'(legibility tax)现象——与仅追求正确性的基线模型相比，准确率会下降。

Method: 提出解耦正确性与可检查性的方法：首先训练一个专注于最大化正确性的求解器模型，然后训练一个'翻译器'模型，将求解器输出的解决方案转换为可检查的形式，同时保留求解器的答案。为此，作者制定了'解耦证明者-验证者博弈'，其均衡对应于忠实且可检查的翻译器。

Result: 该方法通过将正确性和可检查性解耦到不同模型中，避免了传统方法中准确率下降的问题，同时保持了输出的可检查性。

Conclusion: 这种解耦架构为解决大语言模型输出验证难题提供了新思路，能够在维持高准确率的同时实现输出的有效检查，平衡了模型性能与可解释性需求。

Abstract: As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a "translator" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.

</details>


### [67] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentDropoutV2是一种测试时修正或拒绝剪枝框架，通过动态优化多智能体系统信息流，在无需重新训练的情况下使数学任务的平均准确率提升6.3个百分点


<details>
  <summary>Details</summary>
Motivation: 多智能体系统(MAS)虽然擅长复杂推理，但存在个体参与者生成错误信息的级联影响问题；现有解决方案多依赖刚性结构工程或昂贵微调，导致可部署性和适应性受限

Method: 提出AgentDropoutV2测试时框架，作为主动防火墙拦截智能体输出，采用检索增强修正器基于失败驱动指示器池迭代纠错，利用蒸馏失败模式作为先验知识精确定位潜在错误，修剪不可修复输出防止错误传播，并通过回退策略保障系统完整性

Result: 在广泛的数学基准测试中显著提升MAS任务性能，平均准确率提升6.3个百分点，系统展现出鲁棒的泛化能力和适应性，能基于任务难度动态调节修正力度，利用上下文感知指示器解决广泛错误模式

Conclusion: AgentDropoutV2通过无需重新训练的动态信息流优化和错误过滤机制，有效缓解了MAS中的错误级联传播问题，实现了部署能力与适应性的双重提升

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [68] [Evaluating Stochasticity in Deep Research Agents](https://arxiv.org/abs/2602.23271)
*Haotian Zhai,Elias Stengel-Eskin,Pratik Patil,Liu Leqi*

Main category: cs.AI

TL;DR: 本研究系统性地分析了深度研究智能体的随机性问题，建立了评估框架，识别出信息获取、信息压缩和推理三大随机性来源，并提出了缓解策略，在保持研究质量的同时将平均随机性降低了22%。


<details>
  <summary>Details</summary>
Motivation: 尽管DRA在研究质量（如结果准确性）方面有所改进，但其实际部署面临一个关键障碍：随机性。在相同查询下，DRA的重复执行在研究结果、发现和引用方面表现出显著变异性，这种不稳定性阻碍了其在真实场景中的应用。

Method: 1) 将DRAs建模为信息获取马尔可夫决策过程，形式化研究随机性问题；2) 开发评估框架量化系统方差；3) 识别并分析三个随机性来源：信息获取、信息压缩和推理；4) 通过对照实验研究各模块在不同决策步骤的随机性对输出方差的影响；5) 提出结构化输出和基于集成的查询生成等缓解策略。

Result: 研究发现减少随机性可提升研究输出质量，其中推理模块和早期阶段的随机性对输出方差贡献最大。在DeepSearchQA数据集上的实验表明，提出的缓解方法使平均随机性降低22%，同时保持高水平的研究质量。

Conclusion: 通过系统化评估和针对性地处理各随机性源（尤其是推理阶段和早期阶段），可以在不牺牲输出质量的前提下显著提升DRA系统的稳定性和可靠性，为其实际部署提供了可行路径。

Abstract: Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations. In this paper, we formalize the study of stochasticity in DRAs by modeling them as information acquisition Markov Decision Processes. We introduce an evaluation framework that quantifies variance in the system and identify three sources of it: information acquisition, information compression, and inference. Through controlled experiments, we investigate how stochasticity from these modules across different decision steps influences the variance of DRA outputs. Our results show that reducing stochasticity can improve research output quality, with inference and early-stage stochasticity contributing the most to DRA output variance. Based on these findings, we propose strategies for mitigating stochasticity while maintaining output quality via structured output and ensemble-based query generation. Our experiments on DeepSearchQA show that our proposed mitigation methods reduce average stochasticity by 22% while maintaining high research quality.

</details>


### [69] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: CXReasonAgent是一个集成了LLM与临床基础诊断工具的诊断代理，通过使用图像衍生的诊断和视觉证据，实现了基于证据的胸部X光诊断推理。


<details>
  <summary>Details</summary>
Motivation: 胸部X光解释需要多步骤、基于证据的推理，但现有LVLMs生成的回答缺乏忠实的证据支撑，视觉证据有限，且支持新诊断任务需重新训练，限制了其在临床环境中的可靠性和适应性。

Method: 提出CXReasonAgent诊断代理架构，将大型语言模型与临床基础的诊断工具集成，利用图像衍生的诊断和视觉证据执行基于证据的诊断推理。同时构建CXReasonDial基准数据集，包含12个诊断任务的1,946个多轮对话。

Result: 实验表明，CXReasonAgent能够产生忠实地基于证据的回答，相比传统LVLMs提供了更可靠和可验证的诊断推理能力。

Conclusion: 在安全至关重要的临床环境中，集成临床基础的诊断工具对于实现可靠、可验证的诊断推理具有重要意义，为临床辅助诊断系统提供了新的有效方向。

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


### [70] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: ODEBRAIN: 一个基于神经ODE的潜在动态预测框架，用于EEG动力学建模，通过整合时空频特征和连续动力学建模，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统潜在变量方法通过循环架构离散化时间建模连续大脑动力学，导致累积预测误差，且无法捕捉EEG的瞬时非线性特征，需要更有效的方法来处理连续时空动力学和复杂大脑状态的随机变化。

Method: 提出ODEBRAIN框架，将时空频特征整合到频谱图节点中，然后采用神经ODE（Neural Ordinary Differential Equation）建模连续的潜在动力学，使潜在表示能够捕捉任意时间点复杂大脑状态的随机变化。

Result: 大量实验验证表明，ODEBRAIN在预测EEG动力学方面显著优于现有方法，展现出更强的鲁棒性和泛化能力。

Conclusion: 通过引入神经ODE和时空频谱图节点整合，ODEBRAIN成功克服了传统方法的时间离散化限制和累积误差问题，为连续大脑动力学的建模提供了更准确、更鲁棒的解决方案。

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


### [71] [The logic of KM belief update is contained in the logic of AGM belief revision](https://arxiv.org/abs/2602.23302)
*Giacomo Bonanno*

Main category: cs.AI

TL;DR: 论文通过将KM信念更新公理转化为包含三个模态算子（B、>、□）的模态逻辑，并与从AGM信念修正公理转化而来的模态逻辑进行对比，证明了AGM信念修正是KM信念更新的特例。


<details>
  <summary>Details</summary>
Motivation: 建立KM信念更新与AGM信念修正两种信念改变框架之间的形式关系，利用模态逻辑作为比较工具，深入理解两者之间的理论联系。

Method: 1) 将KM信念更新的每个公理对应转化为包含单模态信念算子B、双模态条件算子>和单模态必然算子□的模态逻辑，得到逻辑系统L_KM；
2) 与从AGM信念修正公理转化而来的模态逻辑L_AGM进行比较；
3) 证明L_KM的每个公理都是L_AGM的定理；
4) 对强版KM信念更新，分析L_KM与L_AGM的差异，将其归约为单一公理。

Result: 证明了AGM信念修正是KM信念更新的特例（L_AGM包含L_KM，即L_KM的所有公理都是L_AGM的定理）；对于强版KM信念更新，L_KM与L_AGM的差异可缩小为一个仅处理非意外信息（即最初未被不相信的公式）的单一公理。

Conclusion: 通过模态逻辑形式化分析，建立了KM信念更新与AGM信念修正之间的层级关系：前者是更一般的框架，后者是其特殊情况。这为信念改变理论提供了清晰的逻辑基础，有助于理解不同信念改变机制的内在联系。

Abstract: For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\mathcal L_{AGM}$ and the former by $\mathcal L_{KM}$ we show that every axiom of $\mathcal L_{KM}$ is a theorem of $\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\mathcal L_{KM}$ and $\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.

</details>


### [72] [Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315)
*Sha Hu*

Main category: cs.AI

TL;DR: 提出一种基于重采样的推理方法，通过对输入进行多个不变变换版本的推理并聚合结果，利用认知不确定性的部分独立性来提高推理精度。


<details>
  <summary>Details</summary>
Motivation: 即使是优化过的AI模型，由于认知不确定性（aleatoric）和认知不确定性（epistemic）仍会产生推理错误。研究发现，基于不变变换的多个推理样本显示部分独立性，为提高推理精度和平衡模型规模与性能提供了新思路。

Method: 在已经训练好的AI模型上，对输入应用多个不变变换版本进行推理，然后将这些推理输出聚合为一个更准确的结果。方法利用了推理错误在不变变换下的部分独立性特征。

Result: 该重采样推理方法能够有效提高推理精度，为平衡模型大小和性能提供了新的策略。

Conclusion: 重采样推理方法通过聚合多个变换版本的推理结果，解决了认知不确定性导致的推理错误问题，为提升AI模型推理性能提供了实用的解决方案。

Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

</details>


### [73] [Generalized Rapid Action Value Estimation in Memory-Constrained Environments](https://arxiv.org/abs/2602.23318)
*Aloïs Rautureau,Tristan Cazenave,Éric Piette*

Main category: cs.AI

TL;DR: 本文提出了GRAVE2、GRAVER和GRAVER2三种改进算法，通过两级搜索和节点回收技术，在保持与原GRAVE算法相同游戏实力的同时，大幅降低了内存需求


<details>
  <summary>Details</summary>
Motivation: 原GRAVE算法虽然表现优秀，但在每个节点存储额外的胜/访问统计数据导致内存占用过高，这在内存受限的环境中限制了其实际应用和部署

Method: 提出了三种扩展GRAVE的算法：1) GRAVE2采用两级搜索结构；2) GRAVER采用节点回收技术；3) GRAVER2结合了上述两种技术

Result: 实验表明，这些增强算法能够在保持与GRAVE相当的游戏强度的同时，实现存储节点数量的显著削减

Conclusion: 通过引入两级搜索和节点回收技术，新算法有效解决了GRAVE算法的内存瓶颈问题，使其更适合在内存受限的实际环境中应用

Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.

</details>


### [74] [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329)
*Chen Bo Calvin Zhang,Christina Q. Knight,Nicholas Kruus,Jason Hausenloy,Pedro Medeiros,Nathaniel Li,Aiden Kim,Yury Orlovskiy,Coleman Breen,Bryce Cai,Jasper Götting,Andrew Bo Liu,Samira Nedungadi,Paula Rodriguez,Yannis Yiming He,Mohamed Shaaban,Zifan Wang,Seth Donoughe,Julian Michael*

Main category: cs.AI

TL;DR: 这项研究是一项旨在评估大型语言模型（LLMs）是否能显著提升非专业人士在生物安全相关任务中表现的人类提升研究。在8项复杂的生物安全相关任务中，有LLM访问权限的新手用户比仅使用互联网的新手准确性高出4.16倍，甚至在某些任务中超过了专家表现。


<details>
  <summary>Details</summary>
Motivation: 研究的核心动机是厘清LLMs是否能提升新手用户的能力，这对理解科学加速与双重使用风险至关重要。当前LLMs在生物基准测试中表现良好，但尚不清楚它们是否真正能使人类表现得比仅使用互联网资源更好。

Method: 开展了多模型、多基准的人类提升研究，将8项生物安全相关任务集中的参与者分为两组：有LLM访问权限的新手与仅能使用互联网的新手。参与者在复杂问题上投入充足时间（对于最复杂的任务最多可达13小时）进行工作。

Result: LLM访问带来了显著提升：有LLM的新手比对照组的准确性高出4.16倍（95%置信区间 [2.63, 6.87]）。在4个有专家基准的测试中，有LLM的新手在3项上超过了专家。独立LLMs的表现经常超过LLM辅助的新手，表明用户未能充分发挥LLMs的潜力。大多数参与者（89.6%）报告尽管存在安全措施，获取双重使用相关信息几乎没有困难。

Conclusion: LLMs在以往需要受过训练的专业人士才能完成的生物任务上显著提升了新手的表现，这凸显了除了传统基准测试外，还需要持续进行交互式、人类介入的提升评估，以更好地理解和应对科学加速与安全风险。

Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

</details>


### [75] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 提出一种基于细粒度任务分解的多智能体LLM金融交易框架，在日本股票数据上证明该方法相比粗粒度设计能显著提高风险调整收益，并通过投资组合优化进一步改进性能。分析输出与决策偏好的对齐是系统优化的关键机制。


<details>
  <summary>Details</summary>
Motivation: 现有基于多智能体的自主金融交易系统往往依赖抽象指令，忽略了真实工作流程的复杂性，导致推理性能下降且决策过程不透明，需要一种更贴近实际操作流程的设计方法。

Method: 设计多智能体LLM交易框架，将投资分析显式分解为细粒度任务而非粗粒度指令；在日本股票市场（涵盖价格、财报、新闻和宏观信息）进行泄漏控制的回测评估；利用与股票指数的低相关性和各系统输出方差进行标准投资组合优化。

Result: 细粒度任务分解显著优于传统粗粒度设计，能提高风险调整收益；中间输出分析揭示分析输出与下游决策偏好的对齐是性能关键驱动因素；投资组合优化策略取得更优表现。

Conclusion: 研究为在实际交易场景中应用LLM智能体提供了关于智能体结构设计和任务配置的重要指导，细粒度任务分解和输出对齐是构建高效自主交易系统的关键。

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>
