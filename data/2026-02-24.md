<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Vibe Coding on Trial: Operating Characteristics of Unanimous LLM Juries](https://arxiv.org/abs/2602.18492)
*Muhammad Aziz Ullah,Abdul Serwadda*

Main category: cs.DB

TL;DR: 研究了使用LLM陪审团（委员会）自动审查模型生成SQL查询的效果，证明了强有力的模型组成的小型一致委员会可以在减少错误接受的同时保留较多正确查询。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在编程方面已足够优秀，开发者可以用自然语言描述意图并让工具生成初稿（如GitHub Copilot、Cursor、Replit），但缺乏可靠的方法来判断哪些模型生成的查询是安全的可以接受，无需全部依赖人工审查。

Method: 1) 在82个MySQL文本转SQL任务上使用执行 grounded 协议评估了15个开源模型，识别出表现最好的6个模型；2) 从这6个模型中构建规模为1至6的一致委员会，委员会查看提示词、模式和候选SQL；3) 采用"安全优先"规则：只有当委员会所有成员都认为查询正确时才接受；4) 测量真阳性率、假阳性率和Youden J指标，并按生成器分析委员会性能。

Result: 单个模型作为判断器表现不稳定；由强模型组成的小型一致委员会能够显著减少错误接受，同时仍能通过大量正确查询；委员会的具体组成对性能有显著影响。

Conclusion: LLM陪审团为自动化审查LLM生成的SQL查询提供了可行的解决方案，在安全优先的部署场景中，精心选择的委员会组成能够在保证安全性的同时保持较高的通过率。

Abstract: Large Language Models (LLMs) are now good enough at coding that developers can describe intent in plain language and let the tool produce the first code draft, a workflow increasingly built into tools like GitHub Copilot, Cursor, and Replit. What is missing is a reliable way to tell which model written queries are safe to accept without sending everything to a human. We study the application of an LLM jury to run this review step. We first benchmark 15 open models on 82 MySQL text to SQL tasks using an execution grounded protocol to get a clean baseline of which models are strong. From the six best models we build unanimous committees of sizes 1 through 6 that see the prompt, schema, and candidate SQL and accept it only when every member says it is correct. This rule matches safety first deployments where false accepts are more costly than false rejects. We measure true positive rate, false positive rate and Youden J and we also look at committees per generator. Our results show that single model judges are uneven, that small unanimous committees of strong models can cut false accepts while still passing many good queries, and that the exact committee composition matters significantly.

</details>


### [2] [RDBLearn: Simple In-Context Prediction Over Relational Databases](https://arxiv.org/abs/2602.18495)
*Yanlin Zhang,Linjie Xu,Quan Gan,David Wipf,Minjie Wang*

Main category: cs.DB

TL;DR: RDBLearn是一个将表格上下文学习扩展到关系数据库预测的工具包，通过自动特征化和关系聚合，无需特定任务训练，在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有表格ICL方法主要针对单一表格，而现实世界预测任务往往存在于关系数据库中，预测信号分布在多个关联表格而非单一平面表格，需要将表格ICL能力扩展到关系型预测场景。

Method: 采用简单三步方案：自动特征化每个目标行使用其相关记录的关系聚合；实例化生成的增强表格；在增强表格上运行现成的表格基础模型。提供RDBLearn工具包，具有scikit-learn风格的估计器接口，支持灵活切换不同的表格ICL后端。

Result: 在RelBench和4DBInfer的广泛数据集上，RDBLearn是表现最佳的基础模型方法，有时甚至超越在每个数据集上训练或微调的强监督基线。

Conclusion: 证明了表格ICL可通过简单方案有效扩展到关系预测任务，RDBLearn提供实用便捷的解决方案，使关系数据库预测能轻松利用不同的表格基础模型。

Abstract: Recent advances in tabular in-context learning (ICL) show that a single pretrained model can adapt to new prediction tasks from a small set of labeled examples, avoiding per-task training and heavy tuning. However, many real-world tasks live in relational databases, where predictive signal is spread across multiple linked tables rather than a single flat table. We show that tabular ICL can be extended to relational prediction with a simple recipe: automatically featurize each target row using relational aggregations over its linked records, materialize the resulting augmented table, and run an off-the-shelf tabular foundation model on it. We package this approach in \textit{RDBLearn} (https://github.com/HKUSHXLab/rdblearn), an easy-to-use toolkit with a scikit-learn-style estimator interface that makes it straightforward to swap different tabular ICL backends; a complementary agent-specific interface is provided as well. Across a broad collection of RelBench and 4DBInfer datasets, RDBLearn is the best-performing foundation model approach we evaluate, at times even outperforming strong supervised baselines trained or fine-tuned on each dataset.

</details>


### [3] [The Human Factor in Data Cleaning: Exploring Preferences and Biases](https://arxiv.org/abs/2602.19368)
*Hazim AbdElazim,Shadman Islam,Mostafa Milani*

Main category: cs.DB

TL;DR: 该研究通过受控实验揭示了数据清洗过程中人类决策存在多个系统性的认知偏差，这些偏差在技术熟练的参与者中依然普遍存在，表明偏差源于普遍认知倾向而非缺乏专业知识。


<details>
  <summary>Details</summary>
Motivation: 数据清洗通常被视为纯粹的技术预处理步骤，但实际操作中严重依赖人类的判断。理解人类在数据清洗任务中的认知偏差对于设计更有效的人机协作系统至关重要。

Method: 开展受控调查研究，参与者在人口普查风格场景中执行错误检测、数据修复与填补、以及实体匹配任务，这些场景具有已知的语义有效性。

Result: 发现系统性认知偏差证据：框架效应导致表面格式差异产生误报；锚定和调整偏差使专家线索过度影响决策；代表性启发式使非典型有效组合被误判为错误；实体匹配中表面相似性导致高置信度误报；遗漏偏见使参与者倾向于保留缺失值而非合理填补；自动化推荐的遵从度有限。偏差在技术经验丰富的参与者中持续存在。

Conclusion: 数据清洗中的偏差反映了普遍认知倾向而非缺乏专业 expertise。这推动人机协同清洗系统设计，将表征与语义清晰分离、非强制性地呈现专家或算法建议、并支持对非典型但有效案例的反思性评估。

Abstract: Data cleaning is often framed as a technical preprocessing step, yet in practice it relies heavily on human judgment. We report results from a controlled survey study in which participants performed error detection, data repair and imputation, and entity matching tasks on census-inspired scenarios with known semantic validity. We find systematic evidence for several cognitive bias mechanisms in data cleaning. Framing effects arise when surface-level formatting differences (e.g., capitalization or numeric presentation) increase false-positive error flags despite unchanged semantics. Anchoring and adjustment bias appears when expert cues shift participant decisions beyond parity, consistent with salience and availability effects. We also observe the representativeness heuristic: atypical but valid attribute combinations are frequently flagged as erroneous, and in entity matching tasks, surface similarity produces a substantial false-positive rate with high confidence. In data repair, participants show a robust preference for leaving values missing rather than imputing plausible values, consistent with omission bias. In contrast, automation-aligned switching under strong contradiction does not exceed a conservative rare-error tolerance threshold at the population level, indicating that deference to automated recommendations is limited in this setting. Across scenarios, bias patterns persist among technically experienced participants and across diverse workflow practices, suggesting that bias in data cleaning reflects general cognitive tendencies rather than lack of expertise. These findings motivate human-in-the-loop cleaning systems that clearly separate representation from semantics, present expert or algorithmic recommendations non-prescriptively, and support reflective evaluation of atypical but valid cases.

</details>


### [4] [Breaking the Barriers of Database-Agnostic Transactions](https://arxiv.org/abs/2602.19440)
*Toshihiro Suzuki,Hiroyuki Yamada*

Main category: cs.DB

TL;DR: 论文提出原子单元（Atomic Unit, AU）新概念，解决数据库抽象之上的联邦事务管理挑战，通过利用数据库特定能力的知识实现操作下推，并实现事务元数据与应用数据的高效分离，从而在无需模式迁移的情况下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据库抽象层的联邦事务管理方法面临两个主要挑战：一是抽象层隐藏了数据库特定能力，使得性能优化困难；二是需要将应用数据和事务元数据并置存储，在现有数据库上运行需要进行模式迁移，增加了部署复杂度和成本。

Method: 引入原子单元（AU）概念，通过识别操作可原子执行的边界范围，使联邦事务管理能够积极下推数据库操作以充分利用数据库性能；同时实现事务元数据与应用数据的高效分离；AU在开源的数据库无关联邦事务管理器ScalarDB中实现。

Result: 评估结果表明，集成AU的ScalarDB实现了显著更好的性能和高效的元数据分离，能够在无需模式迁移或显著性能下降的情况下，使联邦事务在现有数据库上运行。

Conclusion: AU通过提供数据库特定能力的先验知识和元数据分离机制，成功解决了现代联邦事务管理方法面临的抽象层限制问题，为在现有数据库基础设施上高效部署联邦事务提供了一种实用解决方案。

Abstract: Federated transaction management has long been used as a method to virtually integrate multiple databases from a transactional perspective, ensuring consistency across the databases. Modern approaches manage transactions on top of a database abstraction to achieve database agnosticism; however, these approaches face several challenges. First, managing transactions on top of a database abstraction makes performance optimization difficult because the abstraction hides away the details of underlying databases, such as database-specific capabilities. Additionally, it requires that application data and the associated transaction metadata be colocated in the same record to allow for efficient updates, necessitating a schema migration to run federated transactions on top of existing databases. This paper introduces a new concept in such database abstraction called Atomicity Unit (AU) to address these challenges. AU enables federated transaction management to aggressively pushdown database operations by making use of the knowledge about the scope within which they can perform operations atomically, fully harnessing the performance of the databases. Moreover, AU enables efficient separation of transaction metadata from application data, allowing federated transactions to run on existing databases without requiring a schema migration or significant performance degradation. In this paper, we describe AU, how AU addresses the challenges, and its implementation within ScalarDB, an open-sourced database-agnostic federated transaction manager. We also present evaluation results demonstrating that ScalarDB with AU achieves significantly better performance and efficient metadata separation.

</details>


### [5] [FuzzySQL: Uncovering Hidden Vulnerabilities in DBMS Special Features with LLM-Driven Fuzzing](https://arxiv.org/abs/2602.19490)
*Yongxin Chen,Zhiyuan Jiang,Chao Zhang,Haoran Xu,Shenglin Xu,Jianping Tang,Zheming Li,Peidai Xie,Yongjun Wang*

Main category: cs.DB

TL;DR: 本文提出了FuzzySQL，一个LLM驱动的自适应模糊测试框架，用于发现数据库管理系统特殊功能中的深层漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统数据库模糊测试技术主要关注语法正确性和通用SQL结构，而忽视了许多关键但晦涩的DBMS功能（如系统级模式GTID、程序化构造PROCEDURE、高级进程命令KILL等）。尽管这些功能很少被典型输入触发，但在边缘条件下执行时可能导致严重的崩溃或安全问题。

Method: FuzzySQL结合了语法引导的SQL生成与逻辑转换渐进式变异这一新颖技术，通过否定条件和重构执行逻辑来探索替代控制路径，生成结构和语义多样化的测试用例。为确保后端的更深执行覆盖，FuzzySQL采用混合错误修复管道，统一基于规则的修补与LLM驱动的语义修复，实现语法和上下文相关错误的自动修正。

Result: 在多个DBMS（包括MySQL、MariaDB、SQLite、PostgreSQL和ClickHouse）上进行评估，共发现37个漏洞，其中7个与测试不足的DBMS特殊功能相关。截至撰文时，已有29个案例被确认，9个获得CVE标识符，14个已被供应商修复，其余漏洞计划在即将发布的版本中修补。

Conclusion: 研究结果凸显了传统模糊测试器在语义功能覆盖方面的局限性，并展示了基于LLM的模糊测试在发现复杂数据库系统中隐藏漏洞的潜力。

Abstract: Traditional database fuzzing techniques primarily focus on syntactic correctness and general SQL structures, leaving critical yet obscure DBMS features, such as system-level modes (e.g., GTID), programmatic constructs (e.g., PROCEDURE), advanced process commands (e.g., KILL), largely underexplored. Although rarely triggered by typical inputs, these features can lead to severe crashes or security issues when executed under edge-case conditions. In this paper, we present FuzzySQL, a novel LLM-powered adaptive fuzzing framework designed to uncover subtle vulnerabilities in DBMS special features. FuzzySQL combines grammar-guided SQL generation with logic-shifting progressive mutation, a novel technique that explores alternative control paths by negating conditions and restructuring execution logic, synthesizing structurally and semantically diverse test cases. To further ensure deeper execution coverage of the back end, FuzzySQL employs a hybrid error repair pipeline that unifies rule-based patching with LLM-driven semantic repair, enabling automatic correction of syntactic and context-sensitive failures. We evaluate FuzzySQL across multiple DBMSs, including MySQL, MariaDB, SQLite, PostgreSQL and Clickhouse, uncovering 37 vulnerabilities, 7 of which are tied to under-tested DBMS special features. As of this writing, 29 cases have been confirmed with 9 assigned CVE identifiers, 14 already fixed by vendors, and additional vulnerabilities scheduled to be patched in upcoming releases. Our results highlight the limitations of conventional fuzzers in semantic feature coverage and demonstrate the potential of LLM-based fuzzing to discover deeply hidden bugs in complex database systems.

</details>


### [6] [Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)](https://arxiv.org/abs/2602.19811)
*Laurent Bindschaedler*

Main category: cs.DB

TL;DR: 本文提出了一种基于OLAP意图签名(O LAP Int ent S ignature)的安全优先中间件缓存系统，将SQL和自然语言查询统一规范化到同一键空间，在TPC-DS、SSB和NYC TLC数据集上实现了82%的缓存命中率。


<details>
  <summary>Details</summary>
Motivation: 分析型工作负载存在大量语义重复，但目前的缓存系统仅按SQL表面形式（文本或AST）键入缓存，导致不同接口（BI工具、笔记本、自然语言接口）之间的重用被碎片化，无法充分利用语义重复。

Method: 引入面向星型结构的仪表板式OLAP的安全优先中间件缓存，将SQL和NL统一规范化为单一键空间——OLAP意图签名，捕获度量、分组级别、过滤器和时间窗口。重用需要严格模式验证和置信度门控NL接受下的精确意图匹配；两种保持正确性的推导（上卷、下钻下滤）扩展覆盖范围而不使用近似匹配。

Result: 在TPC-DS、SSB和NYC TLC共1,395个查询上，本方法实现82%的命中率，对比文本缓存（28%）和AST缓存（56%），且零误命中；推导在层次查询上使命中率翻倍。

Conclusion: 通过OLAP意图签名的统一键空间设计和严格验证机制，显著提升了跨不同查询界面的缓存重用效率，同时通过精确匹配和保持正确性的推导方法确保了查询结果的正确性，为分析型工作负载提供了高置信度的缓存优化方案。

Abstract: Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries.

</details>
