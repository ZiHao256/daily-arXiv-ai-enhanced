<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Vibe Coding on Trial: Operating Characteristics of Unanimous LLM Juries](https://arxiv.org/abs/2602.18492)
*Muhammad Aziz Ullah,Abdul Serwadda*

Main category: cs.DB

TL;DR: 本文研究使用LLM评审团来自动评估模型生成的MySQL查询代码的安全性和正确性，通过多模型一致性表决机制减少错误接受率。


<details>
  <summary>Details</summary>
Motivation: LLM已足够擅长编码，开发者可通过自然语言描述让工具生成代码初稿（如GitHub Copilot、Cursor、Replit等工具）。当前缺乏可靠的方法来自动判断哪些模型生成的查询是安全的、可接受的，无需全部依赖人工审查。

Method: 1)在82个MySQL文本到SQL任务上对15个开源模型进行基准测试，使用基于执行的协议建立性能基线；2)从表现最好的6个模型中构建大小为1-6的一致性评审委员会；3)委员会查看提示词、数据库模式和候选SQL，只有当所有成员都认为正确时才通过；4)测量真阳性率、假阳性率和Youden J指数，并分析不同委员会组合的效果。

Result: 研究发现：单模型评审员表现不稳定；由表现优异模型组成的小型一致性委员会能够显著减少错误接受，同时仍能通过大量正确查询；委员会的具体组成对性能有显著影响。

Conclusion: LLM评审团，特别是由强模型组成的小型一致性委员会，为实现安全第一的代码部署提供了可靠途径，可有效平衡安全性和实用性，减少人工审查负担的同时保持较高的正确代码通过率。

Abstract: Large Language Models (LLMs) are now good enough at coding that developers can describe intent in plain language and let the tool produce the first code draft, a workflow increasingly built into tools like GitHub Copilot, Cursor, and Replit. What is missing is a reliable way to tell which model written queries are safe to accept without sending everything to a human. We study the application of an LLM jury to run this review step. We first benchmark 15 open models on 82 MySQL text to SQL tasks using an execution grounded protocol to get a clean baseline of which models are strong. From the six best models we build unanimous committees of sizes 1 through 6 that see the prompt, schema, and candidate SQL and accept it only when every member says it is correct. This rule matches safety first deployments where false accepts are more costly than false rejects. We measure true positive rate, false positive rate and Youden J and we also look at committees per generator. Our results show that single model judges are uneven, that small unanimous committees of strong models can cut false accepts while still passing many good queries, and that the exact committee composition matters significantly.

</details>


### [2] [The Human Factor in Data Cleaning: Exploring Preferences and Biases](https://arxiv.org/abs/2602.19368)
*Hazim AbdElazim,Shadman Islam,Mostafa Milani*

Main category: cs.DB

TL;DR: 本研究通过受控调查，系统发现了数据清洗过程中的多种认知偏差机制（框架效应、锚定偏差、代表性启发式、遗漏偏差等），这些偏差在技术经验不同的参与者中持续存在，提示需要设计更合理的人机协同清洗系统。


<details>
  <summary>Details</summary>
Motivation: 数据清洗常被视为技术性预处理步骤，但在实践中高度依赖人工判断。本研究旨在揭示人类在数据清洗任务中的认知偏差机制，以及这些偏差是否受技术专长或工作流程实践差异的影响。

Method: 采用受控调查研究方法，让参与者在基于人口普查的场景中执行错误检测、数据修复、缺失值填补和实体匹配等任务，这些场景具有已知的语义有效性。研究观察并量化了人类在这些任务中表现出的系统性认知偏差。

Result: 发现了框架效应（表层格式差异虽不影响语义但增加假阳性错误标记）、锚定与调整偏差（专家线索超出比例地影响决策）、代表性启发式（非典型但有效的属性组合常被误标为错误）、遗漏偏差（优先保留缺失值而非填补合理值）等多种认知偏差。这些偏差在具有技术经验的参与者中持续存在，且不受不同工作流程实践的影响。

Conclusion: 数据清洗中的偏差是一般认知倾向的表现，而非专业能力不足。研究呼吁设计将表示与语义明确分离、以非处方性方式呈现专家或算法建议、并支持对非典型但有效案例进行反思性评估的人机协同清洗系统。

Abstract: Data cleaning is often framed as a technical preprocessing step, yet in practice it relies heavily on human judgment. We report results from a controlled survey study in which participants performed error detection, data repair and imputation, and entity matching tasks on census-inspired scenarios with known semantic validity. We find systematic evidence for several cognitive bias mechanisms in data cleaning. Framing effects arise when surface-level formatting differences (e.g., capitalization or numeric presentation) increase false-positive error flags despite unchanged semantics. Anchoring and adjustment bias appears when expert cues shift participant decisions beyond parity, consistent with salience and availability effects. We also observe the representativeness heuristic: atypical but valid attribute combinations are frequently flagged as erroneous, and in entity matching tasks, surface similarity produces a substantial false-positive rate with high confidence. In data repair, participants show a robust preference for leaving values missing rather than imputing plausible values, consistent with omission bias. In contrast, automation-aligned switching under strong contradiction does not exceed a conservative rare-error tolerance threshold at the population level, indicating that deference to automated recommendations is limited in this setting. Across scenarios, bias patterns persist among technically experienced participants and across diverse workflow practices, suggesting that bias in data cleaning reflects general cognitive tendencies rather than lack of expertise. These findings motivate human-in-the-loop cleaning systems that clearly separate representation from semantics, present expert or algorithmic recommendations non-prescriptively, and support reflective evaluation of atypical but valid cases.

</details>


### [3] [Breaking the Barriers of Database-Agnostic Transactions](https://arxiv.org/abs/2602.19440)
*Toshihiro Suzuki,Hiroyuki Yamada*

Main category: cs.DB

TL;DR: 本文提出了原子性单元这一新的数据库抽象概念，用于解决联邦事务管理中的性能优化和元数据分离问题，并在ScalarDB中实现。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦事务管理方法面临两大挑战：一是基于数据库抽象层进行事务管理屏蔽了底层数据库的特定能力，导致性能优化困难；二是要求应用数据与事务元数据同构存储，使得在现有数据库上运行联邦事务需要进行模式迁移。

Method: 引入原子性单元概念，通过了解原子操作的范围，实现数据库操作的激进下推，充分利用数据库性能；同时支持事务元数据与应用数据的高效分离，避免模式迁移。该概念在开源的数据库无关联邦事务管理器ScalarDB中实现。

Result: 评估结果表明，集成AU功能的ScalarDB取得了显著更好的性能，并实现了高效的元数据分离，能够在不要求模式迁移的情况下在现有数据库上运行联邦事务。

Conclusion: AU概念有效解决了联邦事务管理中的性能优化和元数据分离难题，能够在不对现有数据库进行模式改造的情况下提供高性能的联邦事务支持，具有实际应用价值。

Abstract: Federated transaction management has long been used as a method to virtually integrate multiple databases from a transactional perspective, ensuring consistency across the databases. Modern approaches manage transactions on top of a database abstraction to achieve database agnosticism; however, these approaches face several challenges. First, managing transactions on top of a database abstraction makes performance optimization difficult because the abstraction hides away the details of underlying databases, such as database-specific capabilities. Additionally, it requires that application data and the associated transaction metadata be colocated in the same record to allow for efficient updates, necessitating a schema migration to run federated transactions on top of existing databases. This paper introduces a new concept in such database abstraction called Atomicity Unit (AU) to address these challenges. AU enables federated transaction management to aggressively pushdown database operations by making use of the knowledge about the scope within which they can perform operations atomically, fully harnessing the performance of the databases. Moreover, AU enables efficient separation of transaction metadata from application data, allowing federated transactions to run on existing databases without requiring a schema migration or significant performance degradation. In this paper, we describe AU, how AU addresses the challenges, and its implementation within ScalarDB, an open-sourced database-agnostic federated transaction manager. We also present evaluation results demonstrating that ScalarDB with AU achieves significantly better performance and efficient metadata separation.

</details>


### [4] [FuzzySQL: Uncovering Hidden Vulnerabilities in DBMS Special Features with LLM-Driven Fuzzing](https://arxiv.org/abs/2602.19490)
*Yongxin Chen,Zhiyuan Jiang,Chao Zhang,Haoran Xu,Shenglin Xu,Jianping Tang,Zheming Li,Peidai Xie,Yongjun Wang*

Main category: cs.DB

TL;DR: 提出FuzzySQL：首个针对DBMS特殊特性的LLM驱动自适应模糊测试框架


<details>
  <summary>Details</summary>
Motivation: 传统数据库模糊测试仅关注语法正确性和通用SQL结构，忽视系统级模式（GTID）、程序化构造（PROCEDURE）、高级进程命令（KILL）等关键但模糊的DBMS特殊特性；这些特性在边缘条件下可导致严重崩溃或安全问题

Method: 构建FuzzySQL框架：1）结合语法引导SQL生成与逻辑转换渐进式变异，通过否定条件和重组执行逻辑探索替代控制路径，生成结构和语义多样化的测试用例；2）采用混合错误修复管道，统一基于规则修补与LLM驱动的语义修复，自动修正语法和上下文敏感故障

Result: 在MySQL、MariaDB、SQLite、PostgreSQL和Clickhouse等5个DBMS中发现37个漏洞（其中7个关联未充分测试的特殊特性），29个获确认，9个分配CVE标识符，14个已被供应商修复

Conclusion: 验证了传统模糊器在语义特性覆盖上的局限性，证明基于LLM的模糊测试能够有效发现复杂数据库系统中的深层隐藏缺陷

Abstract: Traditional database fuzzing techniques primarily focus on syntactic correctness and general SQL structures, leaving critical yet obscure DBMS features, such as system-level modes (e.g., GTID), programmatic constructs (e.g., PROCEDURE), advanced process commands (e.g., KILL), largely underexplored. Although rarely triggered by typical inputs, these features can lead to severe crashes or security issues when executed under edge-case conditions. In this paper, we present FuzzySQL, a novel LLM-powered adaptive fuzzing framework designed to uncover subtle vulnerabilities in DBMS special features. FuzzySQL combines grammar-guided SQL generation with logic-shifting progressive mutation, a novel technique that explores alternative control paths by negating conditions and restructuring execution logic, synthesizing structurally and semantically diverse test cases. To further ensure deeper execution coverage of the back end, FuzzySQL employs a hybrid error repair pipeline that unifies rule-based patching with LLM-driven semantic repair, enabling automatic correction of syntactic and context-sensitive failures. We evaluate FuzzySQL across multiple DBMSs, including MySQL, MariaDB, SQLite, PostgreSQL and Clickhouse, uncovering 37 vulnerabilities, 7 of which are tied to under-tested DBMS special features. As of this writing, 29 cases have been confirmed with 9 assigned CVE identifiers, 14 already fixed by vendors, and additional vulnerabilities scheduled to be patched in upcoming releases. Our results highlight the limitations of conventional fuzzers in semantic feature coverage and demonstrate the potential of LLM-based fuzzing to discover deeply hidden bugs in complex database systems.

</details>


### [5] [The Climate Change Knowledge Graph: Supporting Climate Services](https://arxiv.org/abs/2602.19786)
*Miguel Ceriani,Fiorela Ciroku,Alessandro Russo,Massimiliano Schembri,Fai Fung,Neha Mittal,Vito Trianni,Andrea Giovanni Nuzzolese*

Main category: cs.DB

TL;DR: 本文提出了一种气候变化知识图谱，用于整合气候模拟相关的多源数据，弥补了传统检索方式的不足，为研究人员提供更高效、复杂的气候数据查询和分析能力。该知识图谱基于领域专家开发，采用开放许可方式发布，旨在增强气候数据探索能力，辅助气候变化相关的决策过程。


<details>
  <summary>Details</summary>
Motivation: 传统的气候模型数据集检索依赖于现有搜索界面和API，只能通过元数据和社区词汇拼凑获取信息，无法满足气候变化研究与应对策略制定对复杂查询和数据整合的需求。

Method: 构建基于本体论的气候变化知识图谱，整合来自各类气候模拟、场景和配置的多源数据，涉及气候模型、模拟运行、变量、时空域及粒度等多种维度。该方案在领域专家协同下开发，并以开放许可方式发布，保障可互操作性和可访问性。

Result: 建立了支持复杂查询的知识图谱，研究者可通过该图谱灵活检索涵盖模型、模拟数据、变量、空间范围、时间范围等多维度的气候数据。图谱及其底层本体以开放许可发布，可促进跨机构合作与二次开发。

Conclusion: 气候变化知识图谱为气候数据探索提供了统一的语义框架，提升了数据检索与整合的能力，有助于支持在气候变化应对策略制定中做出更为科学和有据的决策。

Abstract: Climate change impacts a broad spectrum of human resources and activities, necessitating the use of climate models to project long-term effects and inform mitigation and adaptation strategies. These models generate multiple datasets by running simulations across various scenarios and configurations, thereby covering a range of potential future outcomes. Currently, researchers rely on traditional search interfaces and APIs to retrieve such datasets, often piecing together information from metadata and community vocabularies. The Climate Change Knowledge Graph is designed to address these challenges by integrating diverse data sources related to climate simulations into a coherent and interoperable knowledge graph. This innovative resource allows for executing complex queries involving climate models, simulations, variables, spatio-temporal domains, and granularities. Developed with input from domain experts, the knowledge graph and its underlying ontology are published with open access license and provide a comprehensive framework that enhances the exploration of climate data, facilitating more informed decision-making in addressing climate change issues.

</details>


### [6] [Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)](https://arxiv.org/abs/2602.19811)
*Laurent Bindschaedler*

Main category: cs.DB

TL;DR: 提出了一种面向仪表板OLAP的安全优先中间件缓存系统,通过将SQL和自然语言统一规范化为OLAP意图签名,在保证正确性的前提下实现82%的缓存命中率,远超基于文本(28%)和AST(56%)的传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统分析工作负载缓存系统仅基于SQL的表面形式(文本或AST)进行键值存储,导致在BI工具、笔记本和自然语言接口等不同前端之间语义重复的查询无法重用,造成缓存碎片化。

Method: 设计了一种安全优先的中间件缓存系统,通过将SQL查询和自然语言请求规范化为统一的"OLAP意图签名"密钥空间,捕获度量指标、分组层次、过滤条件和时间窗口四个语义维度。系统要求严格的模式验证、带置信度门控的自然语言接受度,并引入两种保持正确性的衍生操作(上卷roll-up和下钻filter-down)来扩展覆盖范围,避免近似匹配。

Result: 在TPC-DS、SSB和NYC TLC数据集(共1,395个查询)上的实验表明,系统实现了82%的缓存命中率,相比之下,基于文本的缓存命中率为28%,基于AST的命中率为56%,且实现了零假阳性结果;在层次结构查询上,衍生操作将命中率翻倍。

Conclusion: 通过统一的意图签名空间和严格的正确性保证机制,该系统能够有效跨多种查询接口实现缓存重用,在保持零误报的前提下显著提升缓存效率,为分析型工作负载提供了一种安全可靠的中间件缓存解决方案。

Abstract: Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries.

</details>
